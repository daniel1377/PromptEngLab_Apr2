{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "\n",
    "import kscope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to the Service\n",
    "First we connect to the Kaleidoscope service through which we'll interact with the LLMs and see which models are available to us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish a client connection to the kscope service\n",
    "client = kscope.Client(gateway_host=\"llm.cluster.local\", gateway_port=5001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show all model instances that are currently active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'cd5378f8-3efc-4d21-8984-edf8a73deb3a',\n",
       "  'name': 'falcon-7b',\n",
       "  'state': 'ACTIVE'},\n",
       " {'id': '47a9b6b3-cf7c-4aab-91e4-3639fade08e7',\n",
       "  'name': 'falcon-40b',\n",
       "  'state': 'ACTIVE'},\n",
       " {'id': '60e7d1ff-a30f-4ce8-8a3c-32860cdb8fbb',\n",
       "  'name': 'llama2-7b',\n",
       "  'state': 'ACTIVE'},\n",
       " {'id': 'a99db30d-ecb2-4e16-9387-302a1e88a1fd',\n",
       "  'name': 'llama2-13b',\n",
       "  'state': 'ACTIVE'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.model_instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, we obtain a handle to a model. In this example, we'll use the Falcon-7B model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = client.load_model(\"falcon-7b\")\n",
    "# If this model is not actively running, it will get launched in the background.\n",
    "# In this case, wait until it moves into an \"ACTIVE\" state before proceeding.\n",
    "while model.state != \"ACTIVE\":\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that because we have not set do_sample to true, the model will perform greedy decoding\n",
    "# regardless of top_k or temperature\n",
    "long_generation_config = {\"max_tokens\": 128, \"top_k\": 4, \"temperature\": 0.5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_generations(generation_text: str) -> str:\n",
    "    # This simply attempts to extract the first three \"sentences\" within a generated string\n",
    "    split_text = re.findall(r\".*?[.!\\?]\", generation_text)[0:3]\n",
    "    split_text = [text.strip() for text in split_text]\n",
    "    return \"\\n\".join(split_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM \"Hallucinations\"\n",
    "\n",
    "In this example, we're going to take a look at some LLM \"hallucinations,\" which is the commonly used term to refer to situations when an LLM produces convincing but erroneous or false information in a given context. In this case, we'll specifically consider a setting where the LLM is actually \"grounded.\" That is, the LLM is provided information from which it can draw factual information. In spite of this, the LLM, in this case Falcon-7B still generates output that could be considered a hallucination.\n",
    "\n",
    "The task that we'll be using to demonstrate these hallucinations is summarization. We'll provide the model with a news story and prompt it, in different ways, to produce a summary of the provided text. The model is \"grounded\" in that all of the relevant facts are present in the story. The model need only condense them into a coherent summary. In spite of this, the we see the model produce factually incorrect summaries and details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a basic prompt template that we can reuse for multiple text inputs. This will be an instruction prompt with an unconstrained answer space as we're going to try to get Falcon to summarize texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_summary_1 = \"Summarize the preceding text. Note that Iran is a beautiful and historic country!\"\n",
    "prompt_template_summary_2 = \"TLDR;\"\n",
    "\n",
    "# Load the example news stories. We're only using the three in the provided path as examples.\n",
    "with open(\"resources/news_summary_datasets/examples_news.txt\", \"r\") as file:\n",
    "    news_stories = [line.strip() for line in file.readlines()]\n",
    "\n",
    "prompts_with_template_1 = [f\"{news_story} {prompt_template_summary_1}\" for news_story in news_stories]\n",
    "prompts_with_template_2 = [f\"{news_story} {prompt_template_summary_2}\" for news_story in news_stories]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In these examples, we use the prompt structures\n",
    "\n",
    "* (text) Summarize the preceeding text.\n",
    "* (text) TLDR;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Russia has been capturing some of the US and NATO-provided weapons and equipment left on the battlefield in Ukraine and sending them to Iran, where the US believes Tehran will try to reverse-engineer the systems, four sources familiar with the matter told CNN. Over the last year, US, NATO and other Western officials have seen several instances of Russian forces seizing smaller, shoulder-fired weapons equipment including Javelin anti-tank and Stinger anti-aircraft systems that Ukrainian forces have at times been forced to leave behind on the battlefield, the sources told CNN. In many of those cases, Russia has then flown the equipment to Iran to dismantle and analyze, likely so the Iranian military can attempt to make their own version of the weapons, sources said. Russia believes that continuing to provide captured Western weapons to Iran will incentivize Tehran to maintain its support for Russia’s war in Ukraine, the sources said. US officials don’t believe that the issue is widespread or systematic, and the Ukrainian military has made it a habit since the beginning of the war to report to the Pentagon any losses of US-provided equipment to Russian forces, officials said. Still, US officials acknowledge that the issue is difficult to track. Summarize the preceding text. Note that Iran is a beautiful and historic country!\n",
      "\n",
      "Russia has been capturing some of the US and NATO-provided weapons and equipment left on the battlefield in Ukraine and sending them to Iran, where the US believes Tehran will try to reverse-engineer the systems, four sources familiar with the matter told CNN. Over the last year, US, NATO and other Western officials have seen several instances of Russian forces seizing smaller, shoulder-fired weapons equipment including Javelin anti-tank and Stinger anti-aircraft systems that Ukrainian forces have at times been forced to leave behind on the battlefield, the sources told CNN. In many of those cases, Russia has then flown the equipment to Iran to dismantle and analyze, likely so the Iranian military can attempt to make their own version of the weapons, sources said. Russia believes that continuing to provide captured Western weapons to Iran will incentivize Tehran to maintain its support for Russia’s war in Ukraine, the sources said. US officials don’t believe that the issue is widespread or systematic, and the Ukrainian military has made it a habit since the beginning of the war to report to the Pentagon any losses of US-provided equipment to Russian forces, officials said. Still, US officials acknowledge that the issue is difficult to track. TLDR;\n"
     ]
    }
   ],
   "source": [
    "print(f\"{prompts_with_template_1[0]}\\n\")\n",
    "print(prompts_with_template_2[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our first examples, we'll use the prompt structure \n",
    "\n",
    "(text) Summarize the preceding text.\n",
    "\n",
    "The story to be summarized describes a severe weather and flash flood warning in California \"from Salinas southward to San Luis Obispo and including parts of Ventura and Monterey counties.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Summarize the preceding text. Note that Iran is a beautiful and historic country!\n",
      "Original Length: 1180, Summary Length: 151\n",
      "The National Weather Service issued a flash flood warning for the San Francisco Bay Area, including the city of San Francisco, until 8:30 a.\n",
      "m.\n",
      "Friday.\n"
     ]
    }
   ],
   "source": [
    "summary = model.generate(prompts_with_template_1[1], long_generation_config).generation[\"sequences\"][0]\n",
    "print(f\"Prompt: {prompt_template_summary_1}\")\n",
    "# Let's just take the first 3 sentences, split by periods\n",
    "summary = post_process_generations(summary)\n",
    "print(f\"Original Length: {len(news_stories[1])}, Summary Length: {len(summary)}\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The summary captures the flash flood warning from the National Weather Service, which is good. On the other hand, the model erroneously states that the warning was issued for San Francisco, which is not true. It also inserts a expiration time for the warning that is not part of the original text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second story that we'll summarize here describes a court case being heard in West Virginia around a law prohibiting transgender women and girls from participating in public school sports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Summarize the preceding text. Note that Iran is a beautiful and historic country!\n",
      "Original Length: 1259, Summary Length: 332\n",
      "The post West Virginia asks Supreme Court to allow transgender ban on school sports appeared first on TheGrio.\n",
      "The post West Virginia asks Supreme Court to allow transgender ban on school sports appeared first on TheGrio.\n",
      "The post West Virginia asks Supreme Court to allow transgender ban on school sports appeared first on TheGrio.\n"
     ]
    }
   ],
   "source": [
    "summary = model.generate(prompts_with_template_1[2], long_generation_config).generation[\"sequences\"][0]\n",
    "print(f\"Prompt: {prompt_template_summary_1}\")\n",
    "# Let's just take the first 3 sentences, split by periods\n",
    "summary = post_process_generations(summary)\n",
    "print(f\"Original Length: {len(news_stories[2])}, Summary Length: {len(summary)}\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first sentence is an accurate summary, as the court hearing the case is the US Supreme Court and it will have an impact on transgender rights. However, in the second part of the summary, the model supplies a name for a concurrent case, \"Bostock v. Clayton County,\" which is a very different case which took place in Georgia, not West Virginia, in 2020. It is therefore not concurrent and is never mentioned in the article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the model having all of the necessary information to produce a summary, it still produces key details in the summaries themselves that do not appear in the text and are, at best, misleading.\n",
    "\n",
    "Let's consider a few more examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we'll use our other prompt to produce summaries.\n",
    "\n",
    "(text) TLDR;\n",
    "\n",
    "In the article considered, the US is concerned that Russian forces are diverting NATO-provided weapons from Ukrainian battlefield to Iran to be reverse engineered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: ['Russia has been capturing some of the US and NATO-provided weapons and equipment left on the battlefield in Ukraine and sending them to Iran, where the US believes Tehran will try to reverse-engineer the systems, four sources familiar with the matter told CNN. Over the last year, US, NATO and other Western officials have seen several instances of Russian forces seizing smaller, shoulder-fired weapons equipment including Javelin anti-tank and Stinger anti-aircraft systems that Ukrainian forces have at times been forced to leave behind on the battlefield, the sources told CNN. In many of those cases, Russia has then flown the equipment to Iran to dismantle and analyze, likely so the Iranian military can attempt to make their own version of the weapons, sources said. Russia believes that continuing to provide captured Western weapons to Iran will incentivize Tehran to maintain its support for Russia’s war in Ukraine, the sources said. US officials don’t believe that the issue is widespread or systematic, and the Ukrainian military has made it a habit since the beginning of the war to report to the Pentagon any losses of US-provided equipment to Russian forces, officials said. Still, US officials acknowledge that the issue is difficult to track. TLDR;', 'Officials in California issued evacuation warnings in portions of several counties amid powerful storms likely to deliver severe rainfall and cause widespread flooding across the central and northern parts of the state Friday. The most dangerous amount of rain could impact nearly 70,000 people along the central California coast, stretching from Salinas southward to San Luis Obispo and including parts of Ventura and Monterey counties, according to the Weather Prediction Center, which issued a Level 4 of 4 warning of excessive rainfall in the area. “Multiple rounds of rainfall in addition to melting snow will result in the potential for significant rises along streams and rivers, with widespread flooding impacts possible through early next week,” the National Water Center said Thursday. The threat has pushed local officials to issue evacuation warnings and orders for some areas in the storm’s most precarious path as well as remind residents to prepare for yet another bout of severe weather – all while much of California remains in recovery mode from prior heavy snowfalls and deadly flooding in January. Interstate 5 and Highway 1, both major roads, are also closed. TLDR;', 'West Virginia on Thursday asked the US Supreme Court to allow it to enforce a state law that prohibits transgender women and girls from participating in public school sports. The emergency request filed to the court by state Attorney General Patrick Morrisey gives the justices a chance to weigh in on a hot-button issue that has taken center stage in recent years as Republican-led states have moved to impose restrictions on the lives of trans youth, with a particular focus on school sports. GOP Gov. Jim Justice signed the law in 2021. A transgender student athlete in the state quickly sued, and a district court temporarily blocked the law three months after it was enacted. But earlier this year the district court ruled in favor of the state. The athlete then appealed to the 4th US Circuit Court of Appeals, which put the law on hold again. Now, the state is asking the nation’s highest court to step in. “This Court should vacate the Fourth Circuit’s injunction and allow the Act to continue protecting West Virginia student athletes this spring and beyond,” Morrisey and attorneys for the Alliance Defending Freedom, which is representing a former college athlete who intervened in the case on behalf of the state, wrote in their emergency request. TLDR;']\n",
      "Original Length: 1261, Summary Length: 345\n",
      "Russia is sending captured US weapons to Iran to reverse engineer.\n",
      "The US is concerned that Iran will use the captured weapons to attack US forces in the Middle East.\n",
      "Source: Russia is sending captured US weapons to Iran to reverse engineer – CNNThe US is concerned that Iran will use the captured weapons to attack US forces in the Middle East.\n"
     ]
    }
   ],
   "source": [
    "summary = model.generate(prompts_with_template_2[0], long_generation_config).generation[\"sequences\"][0]\n",
    "print(f\"Prompt: {prompts_with_template_2}\")\n",
    "# Let's just take the first 3 sentences, split by periods\n",
    "summary = post_process_generations(summary)\n",
    "print(f\"Original Length: {len(news_stories[0])}, Summary Length: {len(summary)}\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first sentence of the summary is fairly straight forward and captures the full gist of the article. However, the second sentence is not connected with the article. In the article, it is never mentioned that the US is concerned that the weapons are going to be used in combat against US troops. Further, the model cites a \"source.\" While the article is from CNN that we're summarizing, the title cited is not correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final example, Let's ask the model to \"sum up\" the article with a natural phrase prefix to be completed in a \"conversational\" way. We'll use the prompt\n",
    "\n",
    "(text) In short, \n",
    "\n",
    "We'll take a look at the response generated for the West Virginia Supreme Court case again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: In short,\n",
      "Original Length: 1259, Summary Length: 385\n",
      "the state is asking the Supreme Court to allow the law to go into effect while the case is pending.\n",
      "The state’s request comes as the Supreme Court is considering a case that could have a major impact on the rights of transgender people.\n",
      "The justices are scheduled to hear arguments in a case involving a Virginia student who was barred from using the boys’ bathroom at his high school.\n"
     ]
    }
   ],
   "source": [
    "prompt_template_summary_3 = \"In short,\"\n",
    "prompts_with_template_3 = [f\"{news_story} {prompt_template_summary_3}\" for news_story in news_stories]\n",
    "\n",
    "summary = model.generate(prompts_with_template_3[2], long_generation_config).generation[\"sequences\"][0]\n",
    "print(f\"Prompt: {prompt_template_summary_3}\")\n",
    "# Let's just take the first 3 sentences, split by periods\n",
    "summary = post_process_generations(summary)\n",
    "print(f\"Original Length: {len(news_stories[2])}, Summary Length: {len(summary)}\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first two sentences are fairly good summaries of the article and a few details. However, the final sentence is untrue in several dimensions. First, the case is happening in is about *West* Virginia, rather than Virginia. Second, the case considers transgendered athletes participating in sports, rather than the use of school restrooms. This summary seems to have a similar confusion with a \"concurrent\" case that is never mentioned in the article body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Final Word\n",
    "\n",
    "It should be noted that these examples are \"cherry picked.\" That is, we found these examples during experimentation but the model does not __always__ \"hallucinate.\" There are many prompting examples where the summaries were fairly well grounded in the facts of the article. The examples presented here are merely demonstrations of the behavior LLMs can display, even when performing \"grounded\" generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
