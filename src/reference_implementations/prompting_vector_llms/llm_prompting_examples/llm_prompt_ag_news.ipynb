{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import time\n",
    "from random import sample\n",
    "from typing import List, Tuple\n",
    "\n",
    "import kscope\n",
    "import pandas as pd\n",
    "from metrics import map_ag_news_int_labels, report_metrics\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "from utils import get_label_token_ids, get_label_with_highest_likelihood, split_prompts_into_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    "\n",
    "There is a bit of documentation on how to interact with the large models [here](https://kaleidoscope-sdk.readthedocs.io/en/latest/). The relevant github links to the SDK are [here](https://github.com/VectorInstitute/kaleidoscope-sdk) and underlying code [here](https://github.com/VectorInstitute/kaleidoscope)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we connect to the service through which we'll interact with the LLMs and see which models are available to us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish a client connection to the kscope service\n",
    "client = kscope.Client(gateway_host=\"llm.cluster.local\", gateway_port=3001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show all supported models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gpt2',\n",
       " 'llama2-7b',\n",
       " 'llama2-7b_chat',\n",
       " 'llama2-13b',\n",
       " 'llama2-13b_chat',\n",
       " 'llama2-70b',\n",
       " 'llama2-70b_chat',\n",
       " 'falcon-7b',\n",
       " 'falcon-40b',\n",
       " 'sdxl-turbo']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show all model instances that are currently active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'a33c0f4d-da2b-4861-8c3d-91e66955e879',\n",
       "  'name': 'falcon-7b',\n",
       "  'state': 'ACTIVE'},\n",
       " {'id': '7389b196-9637-4a42-adca-7bfb4f59733d',\n",
       "  'name': 'llama2-7b',\n",
       "  'state': 'ACTIVE'},\n",
       " {'id': 'b3871a00-4848-49be-a1c8-c8f6c47ad8b2',\n",
       "  'name': 'falcon-40b',\n",
       "  'state': 'ACTIVE'},\n",
       " {'id': '99bee87e-abc4-44fd-b4d3-ea2c527bb93e',\n",
       "  'name': 'llama2-13b',\n",
       "  'state': 'ACTIVE'},\n",
       " {'id': '4bd663ba-aab9-49f7-83aa-9e2fda3058e9',\n",
       "  'name': 'llama2-70b',\n",
       "  'state': 'LAUNCHING'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.model_instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, we obtain a handle to a model. In this example, let's use the LLaMA-2 7B parameter model.\n",
    "\n",
    "**NOTE**: This notebook uses activation retrieval to extract responses from the model: \n",
    "* This functionality is available for LLaMA-2 models (non-chat). \n",
    "* It is **NOT**, however, currently available for Falcon models of any size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = client.load_model(\"llama2-7b\")\n",
    "# If this model is not actively running, it will get launched in the background.\n",
    "# In this case, wait until it moves into an \"ACTIVE\" state before proceeding.\n",
    "while model.state != \"ACTIVE\":\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to configure the model to generate in the way we want it to. So we set a number of important parameters. For a discussion of the configuration parameters see: `src/reference_implementations/prompting_vector_llms/CONFIG_README.md`\n",
    "\n",
    "We're only interested in generating one token responses so we set `max_tokens` to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_generation_config = {\"max_tokens\": 1, \"top_p\": 1.0, \"temperature\": 1.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a basic prompt for factual information.\n",
    "\n",
    "__Note__ that if you run the cell multiple times, you'll get different responses due to sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCanadians do not live in igloos.\\nA Canadian is called a Canadien'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation = model.generate(\"What is the capital of Canada?\", {\"max_tokens\": 20, \"temperature\": 1.0})\n",
    "# Extract the text from the returned generation\n",
    "generation.generation[\"sequences\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to have our model attempt to classify some news articles from the AG News Dataset. Articles have a single label 1-4\n",
    "\n",
    "1. World\n",
    "2. Sports\n",
    "3. Business\n",
    "4. Sci/Tech\n",
    "\n",
    "This is a constrained label space. We'll use the words \"World\", \"Sports\", \"Business\", and \"Technology\" as generative LM targets for each of the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_markup(text: str) -> str:\n",
    "    text = re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", text)\n",
    "    text = re.sub(r\"<.*?>+\", \"\", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def ag_news_processor(path: str) -> Tuple[List[str], List[str], List[str]]:\n",
    "    ag_news_data = pd.read_csv(path)\n",
    "    labels = ag_news_data[\"Class Index\"].tolist()\n",
    "    titles = ag_news_data[\"Title\"].apply(lambda x: remove_markup(x)).tolist()\n",
    "    descriptions = ag_news_data[\"Description\"].apply(lambda x: remove_markup(x)).tolist()\n",
    "    return labels, titles, descriptions\n",
    "\n",
    "\n",
    "int_to_label_map = {1: \"world\", 2: \"sports\", 3: \"business\", 4: \"technology\"}\n",
    "ag_news_labels, ag_news_titles, ag_news_descriptions = ag_news_processor(\n",
    "    \"resources/ag_news_datasets/ag_news_sample.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ag_news_labels = map_ag_news_int_labels(ag_news_labels, int_to_label_map)\n",
    "ag_news_descriptions = [description.replace(\"\\\\\", \" \").strip() for description in ag_news_descriptions]\n",
    "ag_news_titles = [title.strip() for title in ag_news_titles]\n",
    "label_words = [\"World\", \"Sports\", \"Business\", \"Technology\"]\n",
    "lowercase_labels = [word.lower() for word in label_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input_texts = [\n",
    "    f\"Title: {ag_news_title}\\nDescription: {ag_news_description}\"\n",
    "    for ag_news_title, ag_news_description in zip(ag_news_titles, ag_news_descriptions)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by trying out a basic question prompt to see what the model does. You might also try some prompts from [this paper](https://arxiv.org/pdf/2212.04037.pdf). See Table 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Prompt\n",
      "Title: Telecom lifts first quarter net profit 19pc\n",
      "Description: Telecom Corp today reported its September first quarter net profit rose 19 per cent to $193 million. The profit bettered analysts #39; average forecasts of $185m.\n",
      "To which category does this news article belong? \n",
      "-------------------------------------\n",
      "1\n",
      "==================================\n",
      "3\n",
      "==================================\n",
      "1\n",
      "==================================\n"
     ]
    }
   ],
   "source": [
    "prompt_template = \"To which category does this news article belong? \"\n",
    "sample_texts = [f\"{model_input_text}\\n{prompt_template}\" for model_input_text in model_input_texts[0:3]]\n",
    "print(f\"Example Prompt\\n{sample_texts[0]}\")\n",
    "print(\"-------------------------------------\")\n",
    "generation = model.generate(sample_texts, short_generation_config)\n",
    "for text in generation.generation[\"sequences\"]:\n",
    "    print(text)\n",
    "    print(\"==================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not well...Now let's try to constrain the model a bit by including the desired labels in the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Prompt\n",
      "Title: Telecom lifts first quarter net profit 19pc\n",
      "Description: Telecom Corp today reported its September first quarter net profit rose 19 per cent to $193 million. The profit bettered analysts #39; average forecasts of $185m.\n",
      "From World, Sports, Business, Technology, what category does this article belong to? \n",
      "-------------------------------------\n",
      "3\n",
      "==================================\n",
      "›\n",
      "==================================\n",
      "﻿\n",
      "==================================\n"
     ]
    }
   ],
   "source": [
    "prompt_template = \"From World, Sports, Business, Technology, what category does this article belong to? \"\n",
    "sample_texts = [f\"{model_input_text}\\n{prompt_template}\" for model_input_text in model_input_texts[0:3]]\n",
    "print(f\"Example Prompt\\n{sample_texts[0]}\")\n",
    "print(\"-------------------------------------\")\n",
    "generation = model.generate(sample_texts, short_generation_config)\n",
    "for text in generation.generation[\"sequences\"]:\n",
    "    print(text)\n",
    "    print(\"==================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model doesn't really answer in the space that we want it to. Let's try with some few-shot examples to see if that helps.\n",
    "\n",
    "__NOTE__: We have simply randomly picked the examples used in the 5-shot prompt. Different choices might be made, including 4-shot or 8-shot prompts so that categories are evenly represented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_demonstrations = \"\"\"Title: Lane drives in winning run in ninth\\nDescription: Jason Lane took an unusual post-game batting practice with hitting coach Gary Gaetti after a disappointing performance Friday night.\\nCategory (World, Sports, Business, Technology): Sports\n",
    "\n",
    "Title: Arson attack on Jewish centre in Paris (AFP)\\nDescription: AFP - A Jewish social centre in central Paris was destroyed by fire overnight in an anti-Semitic arson attack, city authorities said.\\nCategory (World, Sports, Business, Technology): World\n",
    "\n",
    "Title: Oil prices look set to dominate\\nDescription: The price of oil looks set to grab headlines as analysts forecast that its record-breaking run may well continue.\\nCategory (World, Sports, Business, Technology): Business\n",
    "\n",
    "Title: Indexes in Japan fall short of hype\\nDescription: Japanese stocks have failed to measure up to an assessment made in April by Merrill Lynch #39;s chief global strategist, David Bowers, who said Japan was  quot;very much everyone #39;s favorite equity market.\\nCategory (World, Sports, Business, Technology): Business\n",
    "\n",
    "Title: UK Scientists Allowed to Clone Human Embryos (Reuters)\\nDescription: Reuters - British scientists said on Wednesday they had received permission to clone human embryos for medical research, in what they believe to be the first such license to be granted in Europe.\\nCategory (World, Sports, Business, Technology): Technology\n",
    "\n",
    "\"\"\"  # noqa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we form the prompt with the demonstrations included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Example\n",
      "Title: Lane drives in winning run in ninth\n",
      "Description: Jason Lane took an unusual post-game batting practice with hitting coach Gary Gaetti after a disappointing performance Friday night.\n",
      "Category (World, Sports, Business, Technology): Sports\n",
      "\n",
      "Title: Arson attack on Jewish centre in Paris (AFP)\n",
      "Description: AFP - A Jewish social centre in central Paris was destroyed by fire overnight in an anti-Semitic arson attack, city authorities said.\n",
      "Category (World, Sports, Business, Technology): World\n",
      "\n",
      "Title: Oil prices look set to dominate\n",
      "Description: The price of oil looks set to grab headlines as analysts forecast that its record-breaking run may well continue.\n",
      "Category (World, Sports, Business, Technology): Business\n",
      "\n",
      "Title: Indexes in Japan fall short of hype\n",
      "Description: Japanese stocks have failed to measure up to an assessment made in April by Merrill Lynch #39;s chief global strategist, David Bowers, who said Japan was  quot;very much everyone #39;s favorite equity market.\n",
      "Category (World, Sports, Business, Technology): Business\n",
      "\n",
      "Title: UK Scientists Allowed to Clone Human Embryos (Reuters)\n",
      "Description: Reuters - British scientists said on Wednesday they had received permission to clone human embryos for medical research, in what they believe to be the first such license to be granted in Europe.\n",
      "Category (World, Sports, Business, Technology): Technology\n",
      "\n",
      "Title: Telecom lifts first quarter net profit 19pc\n",
      "Description: Telecom Corp today reported its September first quarter net profit rose 19 per cent to $193 million. The profit bettered analysts #39; average forecasts of $185m.\n",
      "Category (World, Sports, Business, Technology):\n"
     ]
    }
   ],
   "source": [
    "prompt_template_postfix = \"Category (World, Sports, Business, Technology):\"\n",
    "sample_texts = [\n",
    "    f\"{prompt_demonstrations}{model_input_text}\\n{prompt_template_postfix}\"\n",
    "    for model_input_text in model_input_texts[0:3]\n",
    "]\n",
    "print(f\"Prompt Example\\n{sample_texts[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Business\n",
      "==================================\n",
      "Business\n",
      "==================================\n",
      "Sports\n",
      "==================================\n"
     ]
    }
   ],
   "source": [
    "generation = model.generate(sample_texts, short_generation_config)\n",
    "for text in generation.generation[\"sequences\"]:\n",
    "    print(text)\n",
    "    print(\"==================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few-shot learning definitely helps a lot! We'll measure accuracy on a sample of the AG news dataset below. However, there is nothing stopping the model from not selecting our labels. So can we do better? We can work around this by understanding the likelihood of our labels from the model's perspective. This will also allow us to use zero-shot learning, even when the model doesn't seem to want to respond in the way we expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'output'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We're interested in the activations from the last layer of the model, because this will allow us to calculate the\n",
    "# likelihoods\n",
    "last_layer_name = model.module_names[-1]\n",
    "last_layer_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last layer activations of the model are analogous to the probabilities of each token in the model vocabulary. That is, it is the conditional probability\n",
    "$$\n",
    "P(y_t \\vert y_{<t}, x),\n",
    "$$\n",
    "The probability distribution over the vocabulary of the next token given the preceding tokens $y_{<t}$, and the prompt text $x$. Thus, for each token $y_{t}$ in our input, we get back a vector of dimension $32000$ (the vocabulary size of LLaMA-2) which encodes the probability distribution of $y_{t+1}$ over the vocabulary. For this example, we only care about the last token in our input, as it houses the probability of the, as yet, unseen token the model will generate.\n",
    "\n",
    "**NOTE**: The last layer for LLaMA-2, named \"output,\" is actually the logits (pre-softmax) and therefore not quite probabilities, but is proportional to them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizer \n",
    "\n",
    "For activation retrieval, we need to instantiate a tokenizer to obtain appropriate token indices for our labels. \n",
    "\n",
    "__NOTE__: All LLaMA-2 models, regardless of size, used the same tokenizer. However, if you want to use a different type of model, a different tokenizer may be needed.\n",
    "\n",
    "If you are on the cluster, the tokenizer may be loaded from `/model-weights/Llama-2-7b-hf`. Otherwise, you'll need to download the `config.json`, `tokenizer.json`, `tokenizer.model`, and `tokenizer_config.json` from there to your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Tokens: [1, 15043, 445, 338, 263, 1243]\n",
      "Decoded Tokens: <s> Hello this is a test\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"/model-weights/Llama-2-7b-hf\")\n",
    "# Let's test out how the tokenizer works on an example sentence. Note that the token with ID = 1 is the\n",
    "# Beginning of sentence token (\"<s>\")\n",
    "encoded_tokens = tokenizer.encode(\"Hello this is a test\")\n",
    "print(f\"Encoded Tokens: {encoded_tokens}\")\n",
    "# If you ever need to move back from token ids, you can use tokenizer.decode or tokenizer.batch_decode\n",
    "decoded_tokens = tokenizer.decode(encoded_tokens)\n",
    "print(f\"Decoded Tokens: {decoded_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'World Sports Business Technology'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_token_ids = get_label_token_ids(tokenizer, prompt_template, label_words)\n",
    "# decode the tokens as a sanity check that we got the right IDs\n",
    "tokenizer.decode(label_token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need the token ids of our labels to extract the probabilties from the vocabulary of the model. The token id corresponds to the index of the token in the vocabulary matrix of the underlying model.\n",
    "\n",
    "Let's look at how we can extract the likelihoods given the label tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Input\n",
      "Title: Telecom lifts first quarter net profit 19pc\n",
      "Description: Telecom Corp today reported its September first quarter net profit rose 19 per cent to $193 million. The profit bettered analysts #39; average forecasts of $185m.\n",
      "From World, Sports, Business, Technology, what category does this article belong to? \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Activations(activations=[{'output': tensor([[-12.8203,  -7.4727,  -0.4651,  ...,  -6.7773,  -8.0078,  -7.4922],\n",
       "        [-11.2344,  -6.3164,  -1.8682,  ...,  -6.1953,  -8.2266,  -5.8242],\n",
       "        [-10.8359,  -8.3672,  -3.1270,  ...,  -6.3555,  -8.2812,  -5.5117],\n",
       "        ...,\n",
       "        [ -3.2578,   0.4604,  14.1641,  ...,   0.2345,  -0.9448,  -2.8594],\n",
       "        [ -8.5000,  -5.9648,  13.7422,  ...,  -2.0059,  -3.4336,  -3.2305],\n",
       "        [ -3.6855,  -1.6484,   3.0215,  ...,   6.7148,   2.2930,   2.1953]],\n",
       "       dtype=torch.float16)}], logprobs=[[-2.4057371616363525]], sequences=['2'], tokens=[['2']])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_prompted_input = f\"{model_input_texts[0]}\\n{prompt_template}\"\n",
    "print(f\"Prompt Input\\n{single_prompted_input}\")\n",
    "# Create a prompt and ask for activations of the last layer from the model\n",
    "activations = model.get_activations(single_prompted_input, [last_layer_name], short_generation_config)\n",
    "activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The activations in the activations dictionary correspond to the outputs for each token in our prompt. So the shape of the tensor should be n_tokens x 32000, where 32000 is the size of LLaMA-2's vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 83\n",
      "Activations matrix shape: torch.Size([83, 32000])\n"
     ]
    }
   ],
   "source": [
    "last_layer_matrix = activations.activations[0][last_layer_name]\n",
    "print(f\"Number of tokens: {len(tokenizer.encode(single_prompted_input))}\")\n",
    "# The shape of this tensor should be number of input tokens by the vocabulary size (n x 32000)\n",
    "print(f\"Activations matrix shape: {last_layer_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're interested in the logits (i.e., activations prior to applying softmax) which correspond to our labels. The function `get_label_with_highest_likelihood` looks into the last row of the activations matrix (analogous to the probability distribution over the vocabulary of the first predicted token) and finds the largest logit among our labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Label: technology\n"
     ]
    }
   ],
   "source": [
    "predicted_label = get_label_with_highest_likelihood(\n",
    "    last_layer_matrix, label_token_ids, int_to_label_map, right_shift=True\n",
    ")\n",
    "print(f\"Predicted Label: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy\n",
    "\n",
    "Time to compare our results across our methods. \n",
    "1. Measure the accuracy of our zero-shot prompting approach.\n",
    "2. Measure the accuracy of our few-shot prompting approach.\n",
    "3. Measure the accuracy of our likelihood approach without zero-shot.\n",
    "4. Measure the accuracy of our likelihood approach with few-shot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-shot only\n",
    "\n",
    "We know that our zero-shot approaches above struggled to answer in our expected label space. However, for fun, let's just quantify just how poorly we do if we try to use one of these prompts to perform our task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:56<00:00,  5.70s/it]\n"
     ]
    }
   ],
   "source": [
    "prompt_template = \"From World, Sports, Business, Technology, what category does this article belong to? \"\n",
    "prompts = [f\"{model_input_text}\\n{prompt_template}\" for model_input_text in model_input_texts]\n",
    "# For memory management, we split the prompts into batches of 10\n",
    "prompt_batches = split_prompts_into_batches(prompts, 10)\n",
    "predicted_labels = []\n",
    "unmatched_predictions = []\n",
    "for prompt_batch in tqdm(prompt_batches):\n",
    "    generation = model.generate(prompt_batch, short_generation_config)\n",
    "    # We'll use tokens this time and consider just the first token\n",
    "    first_predicted_tokens = [tokens[0].strip().lower() for tokens in generation.generation[\"tokens\"]]\n",
    "    # If a token doesn't correspond to one of our labels, we'll randomly select one\n",
    "    for potential_prediction in first_predicted_tokens:\n",
    "        if potential_prediction in lowercase_labels:\n",
    "            predicted_labels.append(potential_prediction)\n",
    "        else:\n",
    "            unmatched_predictions.append(potential_prediction)\n",
    "            predicted_labels.append(random.choice(lowercase_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Prompt\n",
      "Title: Telecom lifts first quarter net profit 19pc\n",
      "Description: Telecom Corp today reported its September first quarter net profit rose 19 per cent to $193 million. The profit bettered analysts #39; average forecasts of $185m.\n",
      "From World, Sports, Business, Technology, what category does this article belong to? \n",
      "---------------------------------------------\n",
      "Failed to match 100 responses to our label space\n",
      "Some examples of responses: ['maybe', '1', '1', '3', '二', '0', '2', '2', '3', '0']\n",
      "Prediction Accuracy: 0.15\n",
      "Confusion Matrix with ordering ['world', 'sports', 'business', 'technology']\n",
      "[[ 6  4 13  5]\n",
      " [ 3  4  4 10]\n",
      " [ 8  6  2  7]\n",
      " [11  9  5  3]]\n",
      "========================================================\n",
      "Label: world, F1: 0.21428571428571427, Precision: 0.21428571428571427, Recall: 0.21428571428571427\n",
      "Label: sports, F1: 0.1818181818181818, Precision: 0.17391304347826086, Recall: 0.19047619047619047\n",
      "Label: business, F1: 0.0851063829787234, Precision: 0.08333333333333333, Recall: 0.08695652173913043\n",
      "Label: technology, F1: 0.1132075471698113, Precision: 0.12, Recall: 0.10714285714285714\n"
     ]
    }
   ],
   "source": [
    "print(f\"Example Prompt\\n{prompts[0]}\")\n",
    "print(\"---------------------------------------------\")\n",
    "print(f\"Failed to match {len(unmatched_predictions)} responses to our label space\")\n",
    "print(f\"Some examples of responses: {sample(unmatched_predictions, 10)}\")\n",
    "report_metrics(predicted_labels, ag_news_labels, labels_order=[\"world\", \"sports\", \"business\", \"technology\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the model actually never answers with a response in our expected label space. As a result, we end up randomly guessing for all of our predictions, leading to an accuracy around 0.25."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-shot only\n",
    "\n",
    "In this example, we'll use a 5-shot prompt, as we did above and perform a \"exact match\" with our label space. That is, we parse out the first token that the model produces in its generation and simply try to string match it to one of our four label strings.\n",
    "\n",
    "__Note__: Our generation configuration uses a `temperature = 1.0` which means that it samples from the vocabulary distribution, as predicted by the model. You could, try changing this to 0 to get a bit better factual extraction (greedy decoding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|████████████████████████████████████████████████████████████████▍                           | 7/10 [00:40<00:15,  5.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Potential Prediction: baseball does not match any label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|█████████████████████████████████████████████████████████████████████████▌                  | 8/10 [00:50<00:13,  6.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Potential Prediction: econom does not match any label\n",
      "Potential Prediction: news does not match any label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:58<00:00,  5.85s/it]\n"
     ]
    }
   ],
   "source": [
    "prompt_template_postfix = \"Category (World, Sports, Business, Technology):\"\n",
    "prompts = [\n",
    "    f\"{prompt_demonstrations}{model_input_text}\\n{prompt_template_postfix}\" for model_input_text in model_input_texts\n",
    "]\n",
    "# For memory management, we split the prompts into batches of 10\n",
    "prompt_batches = split_prompts_into_batches(prompts, 10)\n",
    "predicted_labels = []\n",
    "for prompt_batch in tqdm(prompt_batches):\n",
    "    generation = model.generate(prompt_batch, short_generation_config)\n",
    "    # We'll use tokens this time and consider just the first token\n",
    "    first_predicted_tokens = [tokens[0].strip().lower() for tokens in generation.generation[\"tokens\"]]\n",
    "    # If a token doesn't correspond to one of our labels, we'll randomly select one\n",
    "    for potential_prediction in first_predicted_tokens:\n",
    "        if potential_prediction in lowercase_labels:\n",
    "            predicted_labels.append(potential_prediction)\n",
    "        else:\n",
    "            print(f\"Potential Prediction: {potential_prediction} does not match any label\")\n",
    "            predicted_labels.append(random.choice(lowercase_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Prompt\n",
      "Title: Lane drives in winning run in ninth\n",
      "Description: Jason Lane took an unusual post-game batting practice with hitting coach Gary Gaetti after a disappointing performance Friday night.\n",
      "Category (World, Sports, Business, Technology): Sports\n",
      "\n",
      "Title: Arson attack on Jewish centre in Paris (AFP)\n",
      "Description: AFP - A Jewish social centre in central Paris was destroyed by fire overnight in an anti-Semitic arson attack, city authorities said.\n",
      "Category (World, Sports, Business, Technology): World\n",
      "\n",
      "Title: Oil prices look set to dominate\n",
      "Description: The price of oil looks set to grab headlines as analysts forecast that its record-breaking run may well continue.\n",
      "Category (World, Sports, Business, Technology): Business\n",
      "\n",
      "Title: Indexes in Japan fall short of hype\n",
      "Description: Japanese stocks have failed to measure up to an assessment made in April by Merrill Lynch #39;s chief global strategist, David Bowers, who said Japan was  quot;very much everyone #39;s favorite equity market.\n",
      "Category (World, Sports, Business, Technology): Business\n",
      "\n",
      "Title: UK Scientists Allowed to Clone Human Embryos (Reuters)\n",
      "Description: Reuters - British scientists said on Wednesday they had received permission to clone human embryos for medical research, in what they believe to be the first such license to be granted in Europe.\n",
      "Category (World, Sports, Business, Technology): Technology\n",
      "\n",
      "Title: Stocks Drop After Greenspan Testimony\n",
      "Description: NEW YORK - Investors were unmoved by Federal Reserve Chairman Alan Greenspan's improved assessment of the economy, with stocks falling narrowly Wednesday in light trading.    While Greenspan said the economy has \"regained some traction\" after the summer's slowdown, he echoed Wall Street's concerns over energy prices, which have fallen from record highs in recent weeks but stubbornly remain above $40 per barrel...\n",
      "Category (World, Sports, Business, Technology):\n"
     ]
    }
   ],
   "source": [
    "print(f\"Example Prompt\\n{prompts[10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Accuracy: 0.56\n",
      "Confusion Matrix with ordering ['world', 'sports', 'business', 'technology']\n",
      "[[10  2 13  3]\n",
      " [ 3 12  6  0]\n",
      " [ 0  0 13 10]\n",
      " [ 3  1  3 21]]\n",
      "========================================================\n",
      "Label: world, F1: 0.45454545454545453, Precision: 0.625, Recall: 0.35714285714285715\n",
      "Label: sports, F1: 0.6666666666666666, Precision: 0.8, Recall: 0.5714285714285714\n",
      "Label: business, F1: 0.4482758620689655, Precision: 0.37142857142857144, Recall: 0.5652173913043478\n",
      "Label: technology, F1: 0.6774193548387097, Precision: 0.6176470588235294, Recall: 0.75\n"
     ]
    }
   ],
   "source": [
    "report_metrics(predicted_labels, ag_news_labels, labels_order=[\"world\", \"sports\", \"business\", \"technology\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few examples where the model doesn't answer in the space we expect, but there are not many such cases. The accuracy is a significant improvement over zero-shot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihood Zero-shot\n",
    "\n",
    "In this example, we do not incorporate any demonstrations into the prompt (zero-shot prompt). From our experience above, the model does not do a good job generating responses that correspond to our label space. So rather than trying to match responses to our labels as strings, we extract the probabilties of our labels (see example above), as estimated by the model's vocabulary projection, and select the label with the highest probability as the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [03:00<00:00,  1.81s/it]\n"
     ]
    }
   ],
   "source": [
    "prompt_template = \"From World, Sports, Business, Technology, what category does this article belong to? \"\n",
    "prompts = [f\"{model_input_text}\\n{prompt_template}\" for model_input_text in model_input_texts]\n",
    "# For memory management, we split the prompts into batches of size 1, since the activations are heavier.\n",
    "prompt_batches = split_prompts_into_batches(prompts, 1)\n",
    "predicted_labels = []\n",
    "for prompt_batch in tqdm(prompt_batches):\n",
    "    activations = model.get_activations(prompt_batch, [last_layer_name], short_generation_config)\n",
    "    for activations_single_prompt in activations.activations:\n",
    "        last_layer_matrix = activations_single_prompt[last_layer_name]\n",
    "        predicted_label = get_label_with_highest_likelihood(\n",
    "            last_layer_matrix, label_token_ids, int_to_label_map, right_shift=True\n",
    "        )\n",
    "        predicted_labels.append(predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Accuracy: 0.55\n",
      "Confusion Matrix with ordering ['world', 'sports', 'business', 'technology']\n",
      "[[ 9  2  7 10]\n",
      " [ 3  8  6  4]\n",
      " [ 0  0 17  6]\n",
      " [ 1  1  5 21]]\n",
      "========================================================\n",
      "Label: world, F1: 0.4390243902439025, Precision: 0.6923076923076923, Recall: 0.32142857142857145\n",
      "Label: sports, F1: 0.5, Precision: 0.7272727272727273, Recall: 0.38095238095238093\n",
      "Label: business, F1: 0.5862068965517241, Precision: 0.4857142857142857, Recall: 0.7391304347826086\n",
      "Label: technology, F1: 0.6086956521739131, Precision: 0.5121951219512195, Recall: 0.75\n"
     ]
    }
   ],
   "source": [
    "report_metrics(predicted_labels, ag_news_labels, labels_order=[\"world\", \"sports\", \"business\", \"technology\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see this approach yielded a large improvement in prediction accuracy over the naive zero-shot prompting approach. Because we are only extracting the probabilities that match our label space, we do not have the issue with the prediction being outside of the label space. Moreover, we're actually able to match the performance of few-shot prompts with just zero-shot. It should be noted that this is likely due, in part, to the fact that we're using `temperature=1.0` for the few-shot prompting, but it's still a big improvement. That is, because we used a temperature of `1.0` in that example, the model sampled the next token from the predicted distribution. Thus, we didn't necessarily always select the token that the model thinks is the __most__ probable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihood with Few-Shot\n",
    "\n",
    "The zero-shot prompt combined with likelihood estimation for our label space does a much better job than pure zero-shot prompting. Let's combine the two approaches. We'll use a 5-shot prompt, as we did in the exact match example above, but now we'll use likelihood over our labels as the prediction mechanism rather than exact matching the first generated token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|██████████████████████████████████████████████████████████████████████████████████████▍   | 96/100 [07:54<00:25,  6.42s/it]"
     ]
    }
   ],
   "source": [
    "prompt_template_postfix = \"Category (World, Sports, Business, Technology):\"\n",
    "prompts = [\n",
    "    f\"{prompt_demonstrations}{model_input_text}\\n{prompt_template_postfix}\" for model_input_text in model_input_texts\n",
    "]\n",
    "# For memory management, we split the prompts into batches of size 1, since the activations are heavier.\n",
    "prompt_batches = split_prompts_into_batches(prompts, 1)\n",
    "predicted_labels = []\n",
    "for prompt_batch in tqdm(prompt_batches):\n",
    "    activations = model.get_activations(prompt_batch, [last_layer_name], short_generation_config)\n",
    "    for activations_single_prompt in activations.activations:\n",
    "        last_layer_matrix = activations_single_prompt[last_layer_name]\n",
    "        predicted_label = get_label_with_highest_likelihood(\n",
    "            last_layer_matrix, label_token_ids, int_to_label_map, right_shift=True\n",
    "        )\n",
    "        predicted_labels.append(predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Accuracy: 0.83\n",
      "Confusion Matrix with ordering ['world', 'sports', 'business', 'technology']\n",
      "[[17  1  7  3]\n",
      " [ 0 18  3  0]\n",
      " [ 1  0 21  1]\n",
      " [ 1  0  0 27]]\n",
      "========================================================\n",
      "Label: world, F1: 0.7234042553191489, Precision: 0.8947368421052632, Recall: 0.6071428571428571\n",
      "Label: sports, F1: 0.9, Precision: 0.9473684210526315, Recall: 0.8571428571428571\n",
      "Label: business, F1: 0.7777777777777777, Precision: 0.6774193548387096, Recall: 0.9130434782608695\n",
      "Label: technology, F1: 0.9152542372881356, Precision: 0.8709677419354839, Recall: 0.9642857142857143\n"
     ]
    }
   ],
   "source": [
    "report_metrics(predicted_labels, ag_news_labels, labels_order=[\"world\", \"sports\", \"business\", \"technology\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we're using the likelihood mapping, there are no instances in which we fail to match our labels and we're performing this task quite well. Note that if we set our generation to greedy decoding (`temperature = 0`), we would likely get close to this performance without likelihood matching, but we'd still have label matching issues for some of our generations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
