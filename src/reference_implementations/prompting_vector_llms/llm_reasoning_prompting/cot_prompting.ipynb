{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import kscope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to the Service\n",
    "First we connect to the Kaleidoscope service through which we'll interact with the LLMs and see which models are available to us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish a client connection to the kscope service\n",
    "client = kscope.Client(gateway_host=\"llm.cluster.local\", gateway_port=3001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show all model instances that are currently active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'a33c0f4d-da2b-4861-8c3d-91e66955e879',\n",
       "  'name': 'falcon-7b',\n",
       "  'state': 'ACTIVE'},\n",
       " {'id': '7389b196-9637-4a42-adca-7bfb4f59733d',\n",
       "  'name': 'llama2-7b',\n",
       "  'state': 'ACTIVE'},\n",
       " {'id': 'b486d208-a570-47ed-bb35-9de310f9cd02',\n",
       "  'name': 'llama2-70b',\n",
       "  'state': 'ACTIVE'},\n",
       " {'id': 'b3871a00-4848-49be-a1c8-c8f6c47ad8b2',\n",
       "  'name': 'falcon-40b',\n",
       "  'state': 'ACTIVE'},\n",
       " {'id': '99bee87e-abc4-44fd-b4d3-ea2c527bb93e',\n",
       "  'name': 'llama2-13b',\n",
       "  'state': 'ACTIVE'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.model_instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, we obtain a handle to a model. In this example, let's use the Falcon-40B model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = client.load_model(\"falcon-40b\")\n",
    "model = client.load_model(\"llama2-70b\")                    # Daniel modified\n",
    "# If this model is not actively running, it will get launched in the background.\n",
    "# In this case, wait until it moves into an \"ACTIVE\" state before proceeding.\n",
    "while model.state != \"ACTIVE\":\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we setup the configurations that the model uses for generation and decoding. We are going to use **GREEDY** decoding, as done in the original CoT papers. Greedy decoding means that the model always selects the token with the highest probability as its next word. Falcon requires a `do_sample` argument to be `True` in the configuration to perform anything other than greedy decoding and it defaults to False. So the configurations below are both greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In all contexts we use GREEDY decoding (top k = 1)\n",
    "small_generation_config = {\"max_tokens\": 20, \"top_k\": 1, \"temperature\": 0.8}\n",
    "moderate_generation_config = {\"max_tokens\": 100, \"top_k\": 1, \"temperature\": 0.8}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's ask the model a simple question to start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where is it?\n",
      "Canada's capital is Ottawa, located in the province of Ontario.\n",
      "What is the capital of Canada?\n",
      "The capital of Canada is Ottawa. It is located in the province of Ontario.\n",
      "What is the capital of Canada and what is it's location?\n",
      "The capital of Canada is Ottawa. It is located in the province of Ontario.\n",
      "What province is the capital of Canada in?\n",
      "The capital of Canada is in the province of\n"
     ]
    }
   ],
   "source": [
    "generation = model.generate(\"What is the capital of Canada?\", moderate_generation_config)\n",
    "# Extract the text from the returned generation\n",
    "print(generation.generation[\"sequences\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few-Shot Chain of Thought Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by prompting Falcon-40B to solve some word problems and build up to using the Few-Shot CoT method proposed in [\"Chain-of-Thought Prompting Elicits Reasoning\n",
    "in Large Language Models\"](https://arxiv.org/pdf/2201.11903.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's see what happens if we try to solve some word problems with a zero-shot prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have? \n"
     ]
    }
   ],
   "source": [
    "zero_shot_prompt = (\n",
    "    \"The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have? \"\n",
    ")\n",
    "\n",
    "print(zero_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n",
      "What do you call a Mexican guy that has lost his car? Carlos\n",
      "Q\n"
     ]
    }
   ],
   "source": [
    "generation_example = model.generate(zero_shot_prompt, generation_config=small_generation_config)\n",
    "print(generation_example.generation[\"sequences\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correct answer to this word problem is 9. Zero-shot prompting didn't produce the correct answer.\n",
    "\n",
    "Now let's try performing a standard few-shot prompt to see if that helps the model provide the correct answer in a format that we can extract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\n",
      "A: The answer is 11.\n",
      "\n",
      "Q: Benjamin is taking bottle inventory. He has two cases with 15 bottles in each and one with 7. How many bottles are there in total?\n",
      "A: The answer is 37.\n",
      "\n",
      "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
      "A: The answer is \n"
     ]
    }
   ],
   "source": [
    "few_shot_prompt = (\n",
    "    \"Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis \"\n",
    "    \"balls does he have now?\\nA: The answer is 11.\\n\\nQ: Benjamin is taking bottle inventory. He has two cases with \"\n",
    "    \"15 bottles in each and one with 7. How many bottles are there in total?\\nA: The answer is 37.\\n\\nQ: The \"\n",
    "    \"cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\\nA: \"\n",
    "    \"The answer is \"\n",
    ")\n",
    "print(few_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.\n",
      "\n",
      "Q: At a grocery store, there are 14 apples\n"
     ]
    }
   ],
   "source": [
    "generation_example = model.generate(few_shot_prompt, generation_config=small_generation_config)\n",
    "print(generation_example.generation[\"sequences\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the additional context, the generation process takes a bit longer to perform. Moreover, we are still unable to get the correct answer. On the bright side, the model produces the answer in a way that is extractable.\n",
    "\n",
    "Now, let's try prompting the model with a few-shot CoT prompt, where we provide an example of the kind of reasoning required to answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\n",
      "A: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11.\n",
      "\n",
      "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
      "A:\n"
     ]
    }
   ],
   "source": [
    "few_shot_cot_prompt = (\n",
    "    \"Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis \"\n",
    "    \"balls does he have now?\\nA: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. \"\n",
    "    \"5 + 6 = 11. The answer is 11.\\n\\nQ: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 \"\n",
    "    \"more, how many apples do they have?\\nA:\"\n",
    ")\n",
    "print(few_shot_cot_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we switch to the moderate_generation_config to allow the model to generate additional logic. This takes more time for generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cafeteria started with 23 apples. They used 20 to make lunch and bought 6 more. 23 - 20 = 3. 3 + 6 = 9. The answer is 9.\n",
      "\n",
      "Q: The store sold 12 hats. They got 30 more hats in a new shipment. How many hats do they have now?\n",
      "A: The store started with 12\n"
     ]
    }
   ],
   "source": [
    "generation_example = model.generate(few_shot_cot_prompt, generation_config=moderate_generation_config)\n",
    "print(generation_example.generation[\"sequences\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the model doesn't provide quite as much commentary as we did in the example of logic, it does produce arithmetic that leads to the right answer for the first time.\n",
    "\n",
    "Let's try to compare few-shot prompting with few-shot CoT for slightly different kind of problem. This example is drawn from the AQuA: Algebraic Word Problems task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: John found that the average of 15 numbers is 40. If 10 is added to each number then the mean of the numbers is? Answer Choices: (a) 50 (b) 45 (c) 65 (d) 78 (e) 64\n",
      "A: The answer is (a).\n",
      "\n",
      "Q: The capacity of a tank of dimensions (8 m × 6 m × 2.5 m) is Answer Choices: (a) 120 litres (b) 1200 litres (c) 12000 litres (d) 120000 litres (e) None of these\n",
      "A:\n"
     ]
    }
   ],
   "source": [
    "few_shot_prompt = (\n",
    "    \"Q: John found that the average of 15 numbers is 40. If 10 is added to each number then the mean of \"\n",
    "    \"the numbers is? Answer Choices: (a) 50 (b) 45 (c) 65 (d) 78 (e) 64\\nA: The answer is (a).\\n\\nQ: The capacity of \"\n",
    "    \"a tank of dimensions (8 m × 6 m × 2.5 m) is Answer Choices: (a) 120 litres (b) 1200 litres (c) 12000 litres (d) \"\n",
    "    \"120000 litres (e) None of these\\nA:\"\n",
    ")\n",
    "print(few_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer is (d).\n",
      "\n",
      "Q: If the radius of a cylinder is halved\n"
     ]
    }
   ],
   "source": [
    "generation_example = model.generate(few_shot_prompt, generation_config=small_generation_config)\n",
    "print(generation_example.generation[\"sequences\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correct choice for this problem is (d). While the model successfully selects an answer choice in a way we can parse, it selects the wrong answer.\n",
    "\n",
    "Let's see if we can extract the correct answer with few-shot CoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: John found that the average of 15 numbers is 40. If 10 is added to each number then the mean of the numbers is? Answer Choices: (a) 50 (b) 45 (c) 65 (d) 78 (e) 64\n",
      "A: If 10 is added to each number, then the mean of the numbers also increases by 10. So the new mean would be 50. The answer is (a).\n",
      "\n",
      "Q: The capacity of a tank of dimensions (8 m × 6 m × 2.5 m) is Answer Choices: (a) 120 litres (b) 1200 litres (c) 12000 litres (d) 120000 litres (e) None of these \n",
      "A:\n"
     ]
    }
   ],
   "source": [
    "few_shot_cot_prompt = (\n",
    "    \"Q: John found that the average of 15 numbers is 40. If 10 is added to each number then the mean of the numbers \"\n",
    "    \"is? Answer Choices: (a) 50 (b) 45 (c) 65 (d) 78 (e) 64\\nA: If 10 is added to each number, then the mean of the \"\n",
    "    \"numbers also increases by 10. So the new mean would be 50. The answer is (a).\\n\\nQ: The capacity of \"\n",
    "    \"a tank of dimensions (8 m × 6 m × 2.5 m) is Answer Choices: (a) 120 litres (b) 1200 litres (c) 12000 litres (d) \"\n",
    "    \"120000 litres (e) None of these \\nA:\"\n",
    ")\n",
    "print(few_shot_cot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Volume of the tank = (8 × 6 × 2.5) m³ = 120 m³ Volume of 1 m³ = 1000 litres ∴ Capacity of the tank = (120 × 1000) litres = 120000 litres. The answer is (d).\n",
      "\n",
      "Q: 729 is divided into two parts such that one part is 12\n"
     ]
    }
   ],
   "source": [
    "generation_example = model.generate(few_shot_cot_prompt, generation_config=moderate_generation_config)\n",
    "print(generation_example.generation[\"sequences\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model doesn't quite explain its logic fully, but it does enough to allow the model to select an answer choice consistent with the description of the answer in its response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-Shot Chain of Thought Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be tedious and tricky to form useful and effective reasoning examples. Some research has shown that the choice of reasoning examples in CoT prompting can have a large impact on how well the model accomplishes the downstream task. So let's try a zero-shot CoT approach devised in [\"Large Language Models are Zero-Shot Reasoners\"](https://arxiv.org/pdf/2205.11916.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\n",
      "A: The answer is 11.\n",
      "\n",
      "Q: There are 64 students trying out for the school's trivia teams. If 36 of them didn't get picked for the team and the rest were put into 4 groups, how many students would be in each group?\n",
      "A:\n"
     ]
    }
   ],
   "source": [
    "few_shot_prompt = (\n",
    "    \"Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis \"\n",
    "    \"balls does he have now?\\nA: The answer is 11.\\n\\nQ: There are 64 students trying out for the school's trivia \"\n",
    "    \"teams. If 36 of them didn't get picked for the team and the rest were put into 4 groups, how many students would \"\n",
    "    \"be in each group?\\nA:\"\n",
    ")\n",
    "print(few_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer is 8.\n",
      "\n",
      "Q: If 30 students are chosen to be in\n"
     ]
    }
   ],
   "source": [
    "generation_example = model.generate(few_shot_prompt, generation_config=small_generation_config)\n",
    "print(generation_example.generation[\"sequences\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correct answer to this problem is 7.\n",
    "\n",
    "Perhaps we can extract the correct answer with zero-Shot CoT is split into two stages:\n",
    "1) Reasoning Generation\n",
    "2) Answer Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: There are 64 students trying out for the school's trivia teams. If 36 of them didn't get picked for the team and the rest were put into 4 groups, how many students would be in each group?\n",
      "A: Let’s think step by step.\n"
     ]
    }
   ],
   "source": [
    "reasoning_generation_prompt = (\n",
    "    \"Q: There are 64 students trying out for the school's trivia teams. If 36 of them didn't get picked for the team \"\n",
    "    \"and the rest were put into 4 groups, how many students would be in each group?\\nA: Let’s think step by step.\"\n",
    ")\n",
    "print(reasoning_generation_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The first step is to divide the 64 students by 4.\n",
      "The second step is to subtract the 36 students who didn’t get picked from 16.\n",
      "The answer is 13 students.\n",
      "There are 13 students in each group.\n"
     ]
    }
   ],
   "source": [
    "reasoning_generation = model.generate(\n",
    "    reasoning_generation_prompt, generation_config=moderate_generation_config\n",
    ").generation[\"sequences\"][0]\n",
    "print(reasoning_generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: There are 64 students trying out for the school's trivia teams. If 36 of them didn't get picked for the team and the rest were put into 4 groups, how many students would be in each group?\n",
      "A: Let’s think step by step.\n",
      "The first step is to divide the 64 students by 4.\n",
      "The second step is to subtract the 36 students who didn’t get picked from 16.\n",
      "The answer is 13 students.\n",
      "There are 13 students in each group.\n",
      "Therefore, the answer is\n"
     ]
    }
   ],
   "source": [
    "answer_extraction_prompt = f\"{reasoning_generation_prompt}{reasoning_generation}\\nTherefore, the answer is\"\n",
    "print(answer_extraction_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D.\n",
      "Thanks for the help! I love your site!\n"
     ]
    }
   ],
   "source": [
    "answer_generation = model.generate(answer_extraction_prompt, generation_config=moderate_generation_config).generation[\n",
    "    \"sequences\"\n",
    "][0]\n",
    "print(answer_generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After \"thinking step by step\" the model is able to derive and verbalize the correct answer! It should be noted, that, in general zero-shot CoT underperforms well-constructed few-shot CoT in the results. However, the key is \"well-constructed.\" Zero-shot CoT removes the need for significant logic demonstration engineering."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
