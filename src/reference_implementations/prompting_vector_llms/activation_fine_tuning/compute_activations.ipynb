{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "from pprint import pprint\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import datasets\n",
    "import kscope\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '49e036f3-52f7-462c-91bf-c9450c875c49',\n",
       "  'name': 'llama2-13b',\n",
       "  'state': 'ACTIVE'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Establish a client connection to the kscope service\n",
    "client = kscope.Client(gateway_host=\"llm.cluster.local\", gateway_port=3001)\n",
    "client.model_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = client.load_model(\"llama2-13b\")\n",
    "# If this model is not actively running, it will get launched in the background.\n",
    "# In this case, wait until it moves into an \"ACTIVE\" state before proceeding.\n",
    "while model.state != \"ACTIVE\":\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Generation (__NOTE__ This takes a very long time to run (An hour or two)!)\n",
    "\n",
    "In this notebook, we're going to extract the activations from the final non-pad token of LLaMA-13B for different sets of text inputs. This is done (on the back-end) by inserting hooks into the model that allow for extraction of the model's intermediate latent representations. In this notebook, we'll vary both the layer we extract information from and the input itself to consider the affect that these choices have on the performance of a downstream task. In this case, we'll consider a sampling of the IMDB sentiment analysis task to probe these choices.\n",
    "\n",
    "To start, we need to define a configuration for the model generation. Because we only care about the activations of our input, the configuration is less important. The only thing we really need to do is set the `max_tokens` to 1 so that we don't have to worry about indexing into the right spot in our activation matrix. That is, the activations we care about will just occur in the last slot of the tensor. For a discussion of the configuration parameters see [CONFIG_README.md](../../prompting_vector_llms/CONFIG_README.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = {\"max_tokens\": 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Generation \n",
    "\n",
    "Activation generation is quite easy. We can use the client to query the remote model and explore the various modules. Here, we are listing off all layers of the LLaMA-2 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok_embeddings',\n",
       " 'layers',\n",
       " 'layers.0',\n",
       " 'layers.0.attention',\n",
       " 'layers.0.attention.wq',\n",
       " 'layers.0.attention.wk',\n",
       " 'layers.0.attention.wv',\n",
       " 'layers.0.attention.wo',\n",
       " 'layers.0.feed_forward',\n",
       " 'layers.0.feed_forward.w1',\n",
       " 'layers.0.feed_forward.w2',\n",
       " 'layers.0.feed_forward.w3',\n",
       " 'layers.0.attention_norm',\n",
       " 'layers.0.ffn_norm',\n",
       " 'layers.1',\n",
       " 'layers.1.attention',\n",
       " 'layers.1.attention.wq',\n",
       " 'layers.1.attention.wk',\n",
       " 'layers.1.attention.wv',\n",
       " 'layers.1.attention.wo',\n",
       " 'layers.1.feed_forward',\n",
       " 'layers.1.feed_forward.w1',\n",
       " 'layers.1.feed_forward.w2',\n",
       " 'layers.1.feed_forward.w3',\n",
       " 'layers.1.attention_norm',\n",
       " 'layers.1.ffn_norm',\n",
       " 'layers.2',\n",
       " 'layers.2.attention',\n",
       " 'layers.2.attention.wq',\n",
       " 'layers.2.attention.wk',\n",
       " 'layers.2.attention.wv',\n",
       " 'layers.2.attention.wo',\n",
       " 'layers.2.feed_forward',\n",
       " 'layers.2.feed_forward.w1',\n",
       " 'layers.2.feed_forward.w2',\n",
       " 'layers.2.feed_forward.w3',\n",
       " 'layers.2.attention_norm',\n",
       " 'layers.2.ffn_norm',\n",
       " 'layers.3',\n",
       " 'layers.3.attention',\n",
       " 'layers.3.attention.wq',\n",
       " 'layers.3.attention.wk',\n",
       " 'layers.3.attention.wv',\n",
       " 'layers.3.attention.wo',\n",
       " 'layers.3.feed_forward',\n",
       " 'layers.3.feed_forward.w1',\n",
       " 'layers.3.feed_forward.w2',\n",
       " 'layers.3.feed_forward.w3',\n",
       " 'layers.3.attention_norm',\n",
       " 'layers.3.ffn_norm',\n",
       " 'layers.4',\n",
       " 'layers.4.attention',\n",
       " 'layers.4.attention.wq',\n",
       " 'layers.4.attention.wk',\n",
       " 'layers.4.attention.wv',\n",
       " 'layers.4.attention.wo',\n",
       " 'layers.4.feed_forward',\n",
       " 'layers.4.feed_forward.w1',\n",
       " 'layers.4.feed_forward.w2',\n",
       " 'layers.4.feed_forward.w3',\n",
       " 'layers.4.attention_norm',\n",
       " 'layers.4.ffn_norm',\n",
       " 'layers.5',\n",
       " 'layers.5.attention',\n",
       " 'layers.5.attention.wq',\n",
       " 'layers.5.attention.wk',\n",
       " 'layers.5.attention.wv',\n",
       " 'layers.5.attention.wo',\n",
       " 'layers.5.feed_forward',\n",
       " 'layers.5.feed_forward.w1',\n",
       " 'layers.5.feed_forward.w2',\n",
       " 'layers.5.feed_forward.w3',\n",
       " 'layers.5.attention_norm',\n",
       " 'layers.5.ffn_norm',\n",
       " 'layers.6',\n",
       " 'layers.6.attention',\n",
       " 'layers.6.attention.wq',\n",
       " 'layers.6.attention.wk',\n",
       " 'layers.6.attention.wv',\n",
       " 'layers.6.attention.wo',\n",
       " 'layers.6.feed_forward',\n",
       " 'layers.6.feed_forward.w1',\n",
       " 'layers.6.feed_forward.w2',\n",
       " 'layers.6.feed_forward.w3',\n",
       " 'layers.6.attention_norm',\n",
       " 'layers.6.ffn_norm',\n",
       " 'layers.7',\n",
       " 'layers.7.attention',\n",
       " 'layers.7.attention.wq',\n",
       " 'layers.7.attention.wk',\n",
       " 'layers.7.attention.wv',\n",
       " 'layers.7.attention.wo',\n",
       " 'layers.7.feed_forward',\n",
       " 'layers.7.feed_forward.w1',\n",
       " 'layers.7.feed_forward.w2',\n",
       " 'layers.7.feed_forward.w3',\n",
       " 'layers.7.attention_norm',\n",
       " 'layers.7.ffn_norm',\n",
       " 'layers.8',\n",
       " 'layers.8.attention',\n",
       " 'layers.8.attention.wq',\n",
       " 'layers.8.attention.wk',\n",
       " 'layers.8.attention.wv',\n",
       " 'layers.8.attention.wo',\n",
       " 'layers.8.feed_forward',\n",
       " 'layers.8.feed_forward.w1',\n",
       " 'layers.8.feed_forward.w2',\n",
       " 'layers.8.feed_forward.w3',\n",
       " 'layers.8.attention_norm',\n",
       " 'layers.8.ffn_norm',\n",
       " 'layers.9',\n",
       " 'layers.9.attention',\n",
       " 'layers.9.attention.wq',\n",
       " 'layers.9.attention.wk',\n",
       " 'layers.9.attention.wv',\n",
       " 'layers.9.attention.wo',\n",
       " 'layers.9.feed_forward',\n",
       " 'layers.9.feed_forward.w1',\n",
       " 'layers.9.feed_forward.w2',\n",
       " 'layers.9.feed_forward.w3',\n",
       " 'layers.9.attention_norm',\n",
       " 'layers.9.ffn_norm',\n",
       " 'layers.10',\n",
       " 'layers.10.attention',\n",
       " 'layers.10.attention.wq',\n",
       " 'layers.10.attention.wk',\n",
       " 'layers.10.attention.wv',\n",
       " 'layers.10.attention.wo',\n",
       " 'layers.10.feed_forward',\n",
       " 'layers.10.feed_forward.w1',\n",
       " 'layers.10.feed_forward.w2',\n",
       " 'layers.10.feed_forward.w3',\n",
       " 'layers.10.attention_norm',\n",
       " 'layers.10.ffn_norm',\n",
       " 'layers.11',\n",
       " 'layers.11.attention',\n",
       " 'layers.11.attention.wq',\n",
       " 'layers.11.attention.wk',\n",
       " 'layers.11.attention.wv',\n",
       " 'layers.11.attention.wo',\n",
       " 'layers.11.feed_forward',\n",
       " 'layers.11.feed_forward.w1',\n",
       " 'layers.11.feed_forward.w2',\n",
       " 'layers.11.feed_forward.w3',\n",
       " 'layers.11.attention_norm',\n",
       " 'layers.11.ffn_norm',\n",
       " 'layers.12',\n",
       " 'layers.12.attention',\n",
       " 'layers.12.attention.wq',\n",
       " 'layers.12.attention.wk',\n",
       " 'layers.12.attention.wv',\n",
       " 'layers.12.attention.wo',\n",
       " 'layers.12.feed_forward',\n",
       " 'layers.12.feed_forward.w1',\n",
       " 'layers.12.feed_forward.w2',\n",
       " 'layers.12.feed_forward.w3',\n",
       " 'layers.12.attention_norm',\n",
       " 'layers.12.ffn_norm',\n",
       " 'layers.13',\n",
       " 'layers.13.attention',\n",
       " 'layers.13.attention.wq',\n",
       " 'layers.13.attention.wk',\n",
       " 'layers.13.attention.wv',\n",
       " 'layers.13.attention.wo',\n",
       " 'layers.13.feed_forward',\n",
       " 'layers.13.feed_forward.w1',\n",
       " 'layers.13.feed_forward.w2',\n",
       " 'layers.13.feed_forward.w3',\n",
       " 'layers.13.attention_norm',\n",
       " 'layers.13.ffn_norm',\n",
       " 'layers.14',\n",
       " 'layers.14.attention',\n",
       " 'layers.14.attention.wq',\n",
       " 'layers.14.attention.wk',\n",
       " 'layers.14.attention.wv',\n",
       " 'layers.14.attention.wo',\n",
       " 'layers.14.feed_forward',\n",
       " 'layers.14.feed_forward.w1',\n",
       " 'layers.14.feed_forward.w2',\n",
       " 'layers.14.feed_forward.w3',\n",
       " 'layers.14.attention_norm',\n",
       " 'layers.14.ffn_norm',\n",
       " 'layers.15',\n",
       " 'layers.15.attention',\n",
       " 'layers.15.attention.wq',\n",
       " 'layers.15.attention.wk',\n",
       " 'layers.15.attention.wv',\n",
       " 'layers.15.attention.wo',\n",
       " 'layers.15.feed_forward',\n",
       " 'layers.15.feed_forward.w1',\n",
       " 'layers.15.feed_forward.w2',\n",
       " 'layers.15.feed_forward.w3',\n",
       " 'layers.15.attention_norm',\n",
       " 'layers.15.ffn_norm',\n",
       " 'layers.16',\n",
       " 'layers.16.attention',\n",
       " 'layers.16.attention.wq',\n",
       " 'layers.16.attention.wk',\n",
       " 'layers.16.attention.wv',\n",
       " 'layers.16.attention.wo',\n",
       " 'layers.16.feed_forward',\n",
       " 'layers.16.feed_forward.w1',\n",
       " 'layers.16.feed_forward.w2',\n",
       " 'layers.16.feed_forward.w3',\n",
       " 'layers.16.attention_norm',\n",
       " 'layers.16.ffn_norm',\n",
       " 'layers.17',\n",
       " 'layers.17.attention',\n",
       " 'layers.17.attention.wq',\n",
       " 'layers.17.attention.wk',\n",
       " 'layers.17.attention.wv',\n",
       " 'layers.17.attention.wo',\n",
       " 'layers.17.feed_forward',\n",
       " 'layers.17.feed_forward.w1',\n",
       " 'layers.17.feed_forward.w2',\n",
       " 'layers.17.feed_forward.w3',\n",
       " 'layers.17.attention_norm',\n",
       " 'layers.17.ffn_norm',\n",
       " 'layers.18',\n",
       " 'layers.18.attention',\n",
       " 'layers.18.attention.wq',\n",
       " 'layers.18.attention.wk',\n",
       " 'layers.18.attention.wv',\n",
       " 'layers.18.attention.wo',\n",
       " 'layers.18.feed_forward',\n",
       " 'layers.18.feed_forward.w1',\n",
       " 'layers.18.feed_forward.w2',\n",
       " 'layers.18.feed_forward.w3',\n",
       " 'layers.18.attention_norm',\n",
       " 'layers.18.ffn_norm',\n",
       " 'layers.19',\n",
       " 'layers.19.attention',\n",
       " 'layers.19.attention.wq',\n",
       " 'layers.19.attention.wk',\n",
       " 'layers.19.attention.wv',\n",
       " 'layers.19.attention.wo',\n",
       " 'layers.19.feed_forward',\n",
       " 'layers.19.feed_forward.w1',\n",
       " 'layers.19.feed_forward.w2',\n",
       " 'layers.19.feed_forward.w3',\n",
       " 'layers.19.attention_norm',\n",
       " 'layers.19.ffn_norm',\n",
       " 'layers.20',\n",
       " 'layers.20.attention',\n",
       " 'layers.20.attention.wq',\n",
       " 'layers.20.attention.wk',\n",
       " 'layers.20.attention.wv',\n",
       " 'layers.20.attention.wo',\n",
       " 'layers.20.feed_forward',\n",
       " 'layers.20.feed_forward.w1',\n",
       " 'layers.20.feed_forward.w2',\n",
       " 'layers.20.feed_forward.w3',\n",
       " 'layers.20.attention_norm',\n",
       " 'layers.20.ffn_norm',\n",
       " 'layers.21',\n",
       " 'layers.21.attention',\n",
       " 'layers.21.attention.wq',\n",
       " 'layers.21.attention.wk',\n",
       " 'layers.21.attention.wv',\n",
       " 'layers.21.attention.wo',\n",
       " 'layers.21.feed_forward',\n",
       " 'layers.21.feed_forward.w1',\n",
       " 'layers.21.feed_forward.w2',\n",
       " 'layers.21.feed_forward.w3',\n",
       " 'layers.21.attention_norm',\n",
       " 'layers.21.ffn_norm',\n",
       " 'layers.22',\n",
       " 'layers.22.attention',\n",
       " 'layers.22.attention.wq',\n",
       " 'layers.22.attention.wk',\n",
       " 'layers.22.attention.wv',\n",
       " 'layers.22.attention.wo',\n",
       " 'layers.22.feed_forward',\n",
       " 'layers.22.feed_forward.w1',\n",
       " 'layers.22.feed_forward.w2',\n",
       " 'layers.22.feed_forward.w3',\n",
       " 'layers.22.attention_norm',\n",
       " 'layers.22.ffn_norm',\n",
       " 'layers.23',\n",
       " 'layers.23.attention',\n",
       " 'layers.23.attention.wq',\n",
       " 'layers.23.attention.wk',\n",
       " 'layers.23.attention.wv',\n",
       " 'layers.23.attention.wo',\n",
       " 'layers.23.feed_forward',\n",
       " 'layers.23.feed_forward.w1',\n",
       " 'layers.23.feed_forward.w2',\n",
       " 'layers.23.feed_forward.w3',\n",
       " 'layers.23.attention_norm',\n",
       " 'layers.23.ffn_norm',\n",
       " 'layers.24',\n",
       " 'layers.24.attention',\n",
       " 'layers.24.attention.wq',\n",
       " 'layers.24.attention.wk',\n",
       " 'layers.24.attention.wv',\n",
       " 'layers.24.attention.wo',\n",
       " 'layers.24.feed_forward',\n",
       " 'layers.24.feed_forward.w1',\n",
       " 'layers.24.feed_forward.w2',\n",
       " 'layers.24.feed_forward.w3',\n",
       " 'layers.24.attention_norm',\n",
       " 'layers.24.ffn_norm',\n",
       " 'layers.25',\n",
       " 'layers.25.attention',\n",
       " 'layers.25.attention.wq',\n",
       " 'layers.25.attention.wk',\n",
       " 'layers.25.attention.wv',\n",
       " 'layers.25.attention.wo',\n",
       " 'layers.25.feed_forward',\n",
       " 'layers.25.feed_forward.w1',\n",
       " 'layers.25.feed_forward.w2',\n",
       " 'layers.25.feed_forward.w3',\n",
       " 'layers.25.attention_norm',\n",
       " 'layers.25.ffn_norm',\n",
       " 'layers.26',\n",
       " 'layers.26.attention',\n",
       " 'layers.26.attention.wq',\n",
       " 'layers.26.attention.wk',\n",
       " 'layers.26.attention.wv',\n",
       " 'layers.26.attention.wo',\n",
       " 'layers.26.feed_forward',\n",
       " 'layers.26.feed_forward.w1',\n",
       " 'layers.26.feed_forward.w2',\n",
       " 'layers.26.feed_forward.w3',\n",
       " 'layers.26.attention_norm',\n",
       " 'layers.26.ffn_norm',\n",
       " 'layers.27',\n",
       " 'layers.27.attention',\n",
       " 'layers.27.attention.wq',\n",
       " 'layers.27.attention.wk',\n",
       " 'layers.27.attention.wv',\n",
       " 'layers.27.attention.wo',\n",
       " 'layers.27.feed_forward',\n",
       " 'layers.27.feed_forward.w1',\n",
       " 'layers.27.feed_forward.w2',\n",
       " 'layers.27.feed_forward.w3',\n",
       " 'layers.27.attention_norm',\n",
       " 'layers.27.ffn_norm',\n",
       " 'layers.28',\n",
       " 'layers.28.attention',\n",
       " 'layers.28.attention.wq',\n",
       " 'layers.28.attention.wk',\n",
       " 'layers.28.attention.wv',\n",
       " 'layers.28.attention.wo',\n",
       " 'layers.28.feed_forward',\n",
       " 'layers.28.feed_forward.w1',\n",
       " 'layers.28.feed_forward.w2',\n",
       " 'layers.28.feed_forward.w3',\n",
       " 'layers.28.attention_norm',\n",
       " 'layers.28.ffn_norm',\n",
       " 'layers.29',\n",
       " 'layers.29.attention',\n",
       " 'layers.29.attention.wq',\n",
       " 'layers.29.attention.wk',\n",
       " 'layers.29.attention.wv',\n",
       " 'layers.29.attention.wo',\n",
       " 'layers.29.feed_forward',\n",
       " 'layers.29.feed_forward.w1',\n",
       " 'layers.29.feed_forward.w2',\n",
       " 'layers.29.feed_forward.w3',\n",
       " 'layers.29.attention_norm',\n",
       " 'layers.29.ffn_norm',\n",
       " 'layers.30',\n",
       " 'layers.30.attention',\n",
       " 'layers.30.attention.wq',\n",
       " 'layers.30.attention.wk',\n",
       " 'layers.30.attention.wv',\n",
       " 'layers.30.attention.wo',\n",
       " 'layers.30.feed_forward',\n",
       " 'layers.30.feed_forward.w1',\n",
       " 'layers.30.feed_forward.w2',\n",
       " 'layers.30.feed_forward.w3',\n",
       " 'layers.30.attention_norm',\n",
       " 'layers.30.ffn_norm',\n",
       " 'layers.31',\n",
       " 'layers.31.attention',\n",
       " 'layers.31.attention.wq',\n",
       " 'layers.31.attention.wk',\n",
       " 'layers.31.attention.wv',\n",
       " 'layers.31.attention.wo',\n",
       " 'layers.31.feed_forward',\n",
       " 'layers.31.feed_forward.w1',\n",
       " 'layers.31.feed_forward.w2',\n",
       " 'layers.31.feed_forward.w3',\n",
       " 'layers.31.attention_norm',\n",
       " 'layers.31.ffn_norm',\n",
       " 'layers.32',\n",
       " 'layers.32.attention',\n",
       " 'layers.32.attention.wq',\n",
       " 'layers.32.attention.wk',\n",
       " 'layers.32.attention.wv',\n",
       " 'layers.32.attention.wo',\n",
       " 'layers.32.feed_forward',\n",
       " 'layers.32.feed_forward.w1',\n",
       " 'layers.32.feed_forward.w2',\n",
       " 'layers.32.feed_forward.w3',\n",
       " 'layers.32.attention_norm',\n",
       " 'layers.32.ffn_norm',\n",
       " 'layers.33',\n",
       " 'layers.33.attention',\n",
       " 'layers.33.attention.wq',\n",
       " 'layers.33.attention.wk',\n",
       " 'layers.33.attention.wv',\n",
       " 'layers.33.attention.wo',\n",
       " 'layers.33.feed_forward',\n",
       " 'layers.33.feed_forward.w1',\n",
       " 'layers.33.feed_forward.w2',\n",
       " 'layers.33.feed_forward.w3',\n",
       " 'layers.33.attention_norm',\n",
       " 'layers.33.ffn_norm',\n",
       " 'layers.34',\n",
       " 'layers.34.attention',\n",
       " 'layers.34.attention.wq',\n",
       " 'layers.34.attention.wk',\n",
       " 'layers.34.attention.wv',\n",
       " 'layers.34.attention.wo',\n",
       " 'layers.34.feed_forward',\n",
       " 'layers.34.feed_forward.w1',\n",
       " 'layers.34.feed_forward.w2',\n",
       " 'layers.34.feed_forward.w3',\n",
       " 'layers.34.attention_norm',\n",
       " 'layers.34.ffn_norm',\n",
       " 'layers.35',\n",
       " 'layers.35.attention',\n",
       " 'layers.35.attention.wq',\n",
       " 'layers.35.attention.wk',\n",
       " 'layers.35.attention.wv',\n",
       " 'layers.35.attention.wo',\n",
       " 'layers.35.feed_forward',\n",
       " 'layers.35.feed_forward.w1',\n",
       " 'layers.35.feed_forward.w2',\n",
       " 'layers.35.feed_forward.w3',\n",
       " 'layers.35.attention_norm',\n",
       " 'layers.35.ffn_norm',\n",
       " 'layers.36',\n",
       " 'layers.36.attention',\n",
       " 'layers.36.attention.wq',\n",
       " 'layers.36.attention.wk',\n",
       " 'layers.36.attention.wv',\n",
       " 'layers.36.attention.wo',\n",
       " 'layers.36.feed_forward',\n",
       " 'layers.36.feed_forward.w1',\n",
       " 'layers.36.feed_forward.w2',\n",
       " 'layers.36.feed_forward.w3',\n",
       " 'layers.36.attention_norm',\n",
       " 'layers.36.ffn_norm',\n",
       " 'layers.37',\n",
       " 'layers.37.attention',\n",
       " 'layers.37.attention.wq',\n",
       " 'layers.37.attention.wk',\n",
       " 'layers.37.attention.wv',\n",
       " 'layers.37.attention.wo',\n",
       " 'layers.37.feed_forward',\n",
       " 'layers.37.feed_forward.w1',\n",
       " 'layers.37.feed_forward.w2',\n",
       " 'layers.37.feed_forward.w3',\n",
       " 'layers.37.attention_norm',\n",
       " 'layers.37.ffn_norm',\n",
       " 'layers.38',\n",
       " 'layers.38.attention',\n",
       " 'layers.38.attention.wq',\n",
       " 'layers.38.attention.wk',\n",
       " 'layers.38.attention.wv',\n",
       " 'layers.38.attention.wo',\n",
       " 'layers.38.feed_forward',\n",
       " 'layers.38.feed_forward.w1',\n",
       " 'layers.38.feed_forward.w2',\n",
       " 'layers.38.feed_forward.w3',\n",
       " 'layers.38.attention_norm',\n",
       " 'layers.38.ffn_norm',\n",
       " 'layers.39',\n",
       " 'layers.39.attention',\n",
       " 'layers.39.attention.wq',\n",
       " 'layers.39.attention.wk',\n",
       " 'layers.39.attention.wv',\n",
       " 'layers.39.attention.wo',\n",
       " 'layers.39.feed_forward',\n",
       " 'layers.39.feed_forward.w1',\n",
       " 'layers.39.feed_forward.w2',\n",
       " 'layers.39.feed_forward.w3',\n",
       " 'layers.39.attention_norm',\n",
       " 'layers.39.ffn_norm',\n",
       " 'norm',\n",
       " 'output']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.module_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can select the module names of interest and pass them into a `get_activations` function alongside our set of prompts.\n",
    "\n",
    "__NOTE__: Even though `get_activations` below supports a list of module names as its input, it currently does not support retrieving multiple layer activations at the same time for LLaMA-2. If you would like to retrieve multiple layer activations, please perform multiple calls. For example, if you would like activations from two different layers, perform two calls to `get_activations` with the same set of prompts but different module name inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activations(activations=[{'layers.0': tensor([[ 0.0464,  0.0161,  0.0542,  ...,  0.0118,  0.0105, -0.0007],\n",
      "        [ 0.0547, -0.0118,  0.0457,  ...,  0.0460, -0.0007, -0.0750],\n",
      "        [-0.0071, -0.0102,  0.0236,  ...,  0.0019,  0.0247, -0.0515]],\n",
      "       dtype=torch.float16)}, {'layers.0': tensor([[ 0.0464,  0.0161,  0.0542,  ...,  0.0118,  0.0105, -0.0007],\n",
      "        [ 0.0225,  0.0150,  0.0426,  ...,  0.0200,  0.0557, -0.0128],\n",
      "        [ 0.0478,  0.0051, -0.0265,  ..., -0.0123, -0.0550, -0.0129],\n",
      "        [ 0.0308, -0.0104,  0.0361,  ...,  0.0083,  0.0245,  0.0138],\n",
      "        [ 0.0372,  0.0225,  0.0079,  ..., -0.0589, -0.0757, -0.0204]],\n",
      "       dtype=torch.float16)}], logprobs=[[-0.8916411399841309], [-3.503145456314087]], sequences=['!', ','], tokens=[['!'], [',']])\n",
      "Tensor Shape: torch.Size([3, 5120])\n",
      "Tensor Shape: torch.Size([5, 5120])\n"
     ]
    }
   ],
   "source": [
    "prompts = [\"Hello World\", \"Fizz Buzz\"]\n",
    "\n",
    "module_name = \"layers.0\"\n",
    "\n",
    "activations = model.get_activations(prompts, [module_name], generation_config)\n",
    "pprint(activations)\n",
    "\n",
    "# We sent a batch of 2 prompts to the model.\n",
    "# So a list of length two is returned containing activations for the requested layer\n",
    "for activations_single_prompt in activations.activations:\n",
    "    # For each prompt we extract the activations associated with the target module.\n",
    "    raw_activations = activations_single_prompt[module_name]\n",
    "    # The activations should have shape (number of tokens + 1) x (activation size)\n",
    "    # For example, LLaMA-13B has an embedding dimension for the layer requested of 5120\n",
    "    print(\"Tensor Shape:\", raw_activations.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NOTE__: In the code below, we're going to only use batch sizes of 1 to ensure memory management on the backend doesn't get out of hand and slow the model down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizer \n",
    "\n",
    "For activation retrieval, we need to instantiate a tokenizer to obtain appropriate token indices for our labels. \n",
    "\n",
    "__NOTE__: All LLaMA-2 models, regardless of size, used the same tokenizer. However, if you want to use a different type of model, a different tokenizer may be needed.\n",
    "\n",
    "If you are on the cluster, the tokenizer may be loaded from `/model-weights/Llama-2-7b-hf`. Otherwise, you'll need to download the `config.json`, `tokenizer.json`, `tokenizer.model`, and `tokenizer_config.json` from there to your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Tokens: [1, 383, 4981, 350, 18813]\n",
      "Decoded Tokens: ['<s>', 'F', 'izz', 'B', 'uzz']\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer prepares the input of the model. LLaMA models of all sizes use the same underlying tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/model-weights/Llama-2-7b-hf\")\n",
    "# Let's test out how the tokenizer works on an example sentence. Note that the token with ID = 1 is the\n",
    "# Beginning of sentence token (\"BOS\")\n",
    "encoded_tokens = tokenizer.encode(\"Fizz Buzz\")\n",
    "print(f\"Encoded Tokens: {encoded_tokens}\")\n",
    "# If you ever need to move back from token ids, you can use tokenizer.decode or tokenizer.batch_decode\n",
    "decoded_tokens = [tokenizer.decode(encoded_token) for encoded_token in encoded_tokens]\n",
    "print(f\"Decoded Tokens: {decoded_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen above, the second example, \"Fizz Buzz\", is tokenized as [\"F\", \"izz\", \"B\", 'uzz]. So we receive a tensor with 5 rows (one for each token and a final one for the next token to be generated) and 5120 columns (the hidden dimension of LLaMA-2-13B)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a proof of concept of the few-shot abilities of LLMs, we'll only use a small training dataset and will only perform validation using a small test subset for compute efficiency.\n",
    "\n",
    "* Training set: 100 randomly sampled training examples\n",
    "* Test set: 300 randomly sample test examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "imdb = datasets.load_dataset(\"imdb\")\n",
    "train_size = 100\n",
    "test_size = 300\n",
    "n_demonstrations = 5\n",
    "\n",
    "activation_save_path = \"./resources/llama2_13b_activations\"\n",
    "\n",
    "small_train_dataset = imdb[\"train\"].shuffle(seed=42).select([i for i in list(range(train_size))])\n",
    "small_test_dataset = imdb[\"test\"].shuffle(seed=42).select([i for i in list(range(test_size))])\n",
    "# We're going to be experimenting with the affect that prompting the model for the task we care about has on a\n",
    "# classifier trained on the activations in terms of performance. So we will construct demonstrations by randomly\n",
    "# selecting a set of 5 examples from the training set to serve this purpose.\n",
    "small_demonstration_set = imdb[\"train\"].shuffle(seed=42).select([i for i in list(range(n_demonstrations))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split a list into a tuple of lists of a fixed size\n",
    "def batcher(prompts: List[str], batch_size: int) -> Dataset:\n",
    "    return (prompts[pos : pos + batch_size] for pos in range(0, len(prompts), batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're running a lot of activation retrievals. Once in a while there is a json decoding or triton error. If that\n",
    "# happens, we retry the activations request.\n",
    "def get_activations_with_retries(prompt: str, layers: List[str], config: Dict[str, Any], retries: int = 10) -> Any:\n",
    "    retries_required = 0\n",
    "    for _ in range(retries):\n",
    "        try:\n",
    "            activations = model.get_activations(prompt, layers, config)\n",
    "            if retries_required > 0:\n",
    "                print(f\"Something went wrong in activation retrieval. Needed {retries_required} retries\")\n",
    "            return activations\n",
    "        except Exception as e:  # noqa: F841\n",
    "            retries_required += 1\n",
    "    raise ValueError(\"Exceeded retry limit. Exiting Process\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_input_text(text: str, truncation_length: int) -> str:\n",
    "    # If text is longer than truncation length, split by space, take the last truncation_length tokens\n",
    "    split_text = text.split(\" \")\n",
    "    if len(split_text) > truncation_length:\n",
    "        text = \" \".join(split_text[-truncation_length:])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Text Activations\n",
    "\n",
    "Let's start by getting the activations associated with the raw review text. We'll do activations for the text coupled with a prompt below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset_activations(\n",
    "    split: str,\n",
    "    inputs: List[str],\n",
    "    labels: List[int],\n",
    "    module_name: str,\n",
    "    pickle_name: str,\n",
    ") -> None:\n",
    "    print(\"Generating Activations with Prompts: \" + split)\n",
    "\n",
    "    parsed_activations = []\n",
    "    # Using a batch size of 1 as activation fetching is heavy\n",
    "    batches = list(batcher(inputs, 1))\n",
    "    for input_batch in tqdm(batches):\n",
    "        # Getting activations for each input batch. For an example of how get_activations works, see beginning of this\n",
    "        # notebook.\n",
    "        input_batch = [truncate_input_text(input, 256) for input in input_batch]\n",
    "        raw_activations = get_activations_with_retries(input_batch, [module_name], generation_config).activations\n",
    "        for raw_activation in raw_activations:\n",
    "            # We will be performing classification on the last token non-pad token of the sequence. This is common\n",
    "            # practice for auto-regressive models (e.g. OPT, Falcon, LLaMA-2). So we only keep the last row of the\n",
    "            # activation matrix.\n",
    "            parsed_activations.append(raw_activation[module_name][-1].float())\n",
    "\n",
    "    cached_activations = {\"activations\": parsed_activations, \"labels\": labels}\n",
    "\n",
    "    with open(os.path.join(activation_save_path, f\"{split}{pickle_name}.pkl\"), \"wb\") as handle:\n",
    "        pickle.dump(cached_activations, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation Activations for Module Name layers.10\n",
      "Generating Activations with Prompts: train\n",
      "Something went wrong in activation retrieval. Needed 3 retries\n",
      "Batch 50 Completed\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Batch 100 Completed\n",
      "Generating Activations with Prompts: test\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Batch 50 Completed\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Batch 100 Completed\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Something went wrong in activation retrieval. Needed 2 retries\n",
      "Batch 150 Completed\n",
      "Something went wrong in activation retrieval. Needed 3 retries\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Batch 200 Completed\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Something went wrong in activation retrieval. Needed 3 retries\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Batch 250 Completed\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Batch 300 Completed\n",
      "Generation Activations for Module Name layers.20\n",
      "Generating Activations with Prompts: train\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Something went wrong in activation retrieval. Needed 2 retries\n",
      "Batch 50 Completed\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Something went wrong in activation retrieval. Needed 2 retries\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Batch 100 Completed\n",
      "Generating Activations with Prompts: test\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Something went wrong in activation retrieval. Needed 2 retries\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Something went wrong in activation retrieval. Needed 2 retries\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Batch 50 Completed\n",
      "Something went wrong in activation retrieval. Needed 3 retries\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Batch 100 Completed\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Something went wrong in activation retrieval. Needed 3 retries\n",
      "Something went wrong in activation retrieval. Needed 3 retries\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Batch 150 Completed\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Batch 200 Completed\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Something went wrong in activation retrieval. Needed 2 retries\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Batch 250 Completed\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Batch 300 Completed\n",
      "Generation Activations for Module Name layers.30\n",
      "Generating Activations with Prompts: train\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Batch 50 Completed\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Something went wrong in activation retrieval. Needed 2 retries\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Batch 100 Completed\n",
      "Generating Activations with Prompts: test\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Something went wrong in activation retrieval. Needed 2 retries\n",
      "Something went wrong in activation retrieval. Needed 3 retries\n",
      "Something went wrong in activation retrieval. Needed 2 retries\n",
      "Batch 50 Completed\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Batch 100 Completed\n",
      "Something went wrong in activation retrieval. Needed 2 retries\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Batch 150 Completed\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Batch 200 Completed\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Something went wrong in activation retrieval. Needed 2 retries\n",
      "Batch 250 Completed\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Something went wrong in activation retrieval. Needed 3 retries\n",
      "Batch 300 Completed\n",
      "Generation Activations for Module Name layers.39\n",
      "Generating Activations with Prompts: train\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Something went wrong in activation retrieval. Needed 2 retries\n",
      "Something went wrong in activation retrieval. Needed 2 retries\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Batch 50 Completed\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Something went wrong in activation retrieval. Needed 2 retries\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Batch 100 Completed\n",
      "Generating Activations with Prompts: test\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Batch 50 Completed\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Something went wrong in activation retrieval. Needed 2 retries\n",
      "Batch 100 Completed\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Something went wrong in activation retrieval. Needed 4 retries\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Batch 150 Completed\n",
      "Something went wrong in activation retrieval. Needed 2 retries\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Something went wrong in activation retrieval. Needed 2 retries\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Batch 200 Completed\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Something went wrong in activation retrieval. Needed 3 retries\n",
      "Batch 250 Completed\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Something went wrong in activation retrieval. Needed 1 retries\n",
      "Batch 300 Completed\n"
     ]
    }
   ],
   "source": [
    "layer_numbers = [\"10\", \"20\", \"30\", \"39\"]\n",
    "module_names = [f\"layers.{layer_number}\" for layer_number in layer_numbers]\n",
    "\n",
    "train_labels = small_train_dataset[\"label\"]\n",
    "test_labels = small_test_dataset[\"label\"]\n",
    "\n",
    "assert len(module_names) == len(layer_numbers)\n",
    "\n",
    "for module_name, layer_number in zip(module_names, layer_numbers):\n",
    "    print(f\"Generation Activations for Module Name {module_name}\")\n",
    "    generate_dataset_activations(\n",
    "        \"train\", small_train_dataset[\"text\"], train_labels, module_name, f\"_activations_demo_{layer_number}\"\n",
    "    )\n",
    "    generate_dataset_activations(\n",
    "        \"test\", small_test_dataset[\"text\"], test_labels, module_name, f\"_activations_demo_{layer_number}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Conditioned Activations\n",
    "\n",
    "Now let's generate activations pre-conditioned with an instruction and a few demonstrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_demonstrations(instruction: str, demonstration_set: Dataset) -> str:\n",
    "    label_int_to_str = {0: \"negative\", 1: \"positive\"}\n",
    "    demonstration = f\"{instruction}\"\n",
    "    demo_texts = demonstration_set[\"text\"]\n",
    "    demo_labels = demonstration_set[\"label\"]\n",
    "    for text, label in zip(demo_texts, demo_labels):\n",
    "        # truncate the text in case it is very long (cutting the first part of text)\n",
    "        text = truncate_input_text(text, 64)\n",
    "        demonstration = f\"{demonstration}\\nText: {text}\\nSentiment: {label_int_to_str[label]}\"\n",
    "    return f\"{demonstration}\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompts(texts: List[str], demonstration: str) -> List[str]:\n",
    "    truncated_texts = [truncate_input_text(text, 128) for text in texts]\n",
    "    return [f\"{demonstration}Text: {text} The sentiment is\" for text in truncated_texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we show the demonstration structure (based on 5 examples) and what each prompt passed to LLaMA-2 looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demonstration:\n",
      "Classify the sentiment of the text.\n",
      "Text: like to compare, to judge, to evaluate. How about just enjoying? Funny thing too, people writing Fortier looks American but, on the other hand, arguing they prefer American series (!!!). Maybe it's the language, or the spirit, but I think this series is more English than American. By the way, the actors are really good and funny. The acting is not superficial at all...\n",
      "Sentiment: positive\n",
      "Text: favorite song is sung by the King, Hank (bing Crosby) and Sir \"Saggy\" Sagamore. OVerall a great family movie or even a great Date movie. This is a movie you can watch over and over again. The princess played by Rhonda Fleming is gorgeous. I love this movie!! If you liked Danny Kaye in the Court Jester then you will definitely like this movie.\n",
      "Sentiment: positive\n",
      "Text: comprehensible reason for the engagement in South Asia. And for that matter also the reason for every single US-American soldier that was there. Instead, Rambo gets to take revenge for the wounds of a whole nation. It would have been better to work on how to deal with the memories, rather than suppressing them. \"Do we get to win this time?\" Yes, you do.\n",
      "Sentiment: negative\n",
      "Text: employ a political spoils system. There's nothing noble in that. The Missourians could have easily traveled east and joined the Confederate Army.<br /><br />It seems to me that the story has nothing to do with ambiguity. When Jake leaves the Bushwhackers, it's not because he saw error in his way, he certainly doesn't give himself over to the virtue of the cause of abolition.\n",
      "Sentiment: positive\n",
      "Text: this finally drives home the film's other big flaw: lack of originality. In this review, I realize it's been far too easy to reference many other films. Granted, this film is an improvement on most of them, but still. *The Secret Lives of Dentists* is worth seeing, but don't get too excited about it. (Not that you were all that excited, anyway. I guess.)\n",
      "Sentiment: negative\n",
      "\n",
      "Prompt Example:\n",
      "Classify the sentiment of the text.\n",
      "Text: like to compare, to judge, to evaluate. How about just enjoying? Funny thing too, people writing Fortier looks American but, on the other hand, arguing they prefer American series (!!!). Maybe it's the language, or the spirit, but I think this series is more English than American. By the way, the actors are really good and funny. The acting is not superficial at all...\n",
      "Sentiment: positive\n",
      "Text: favorite song is sung by the King, Hank (bing Crosby) and Sir \"Saggy\" Sagamore. OVerall a great family movie or even a great Date movie. This is a movie you can watch over and over again. The princess played by Rhonda Fleming is gorgeous. I love this movie!! If you liked Danny Kaye in the Court Jester then you will definitely like this movie.\n",
      "Sentiment: positive\n",
      "Text: comprehensible reason for the engagement in South Asia. And for that matter also the reason for every single US-American soldier that was there. Instead, Rambo gets to take revenge for the wounds of a whole nation. It would have been better to work on how to deal with the memories, rather than suppressing them. \"Do we get to win this time?\" Yes, you do.\n",
      "Sentiment: negative\n",
      "Text: employ a political spoils system. There's nothing noble in that. The Missourians could have easily traveled east and joined the Confederate Army.<br /><br />It seems to me that the story has nothing to do with ambiguity. When Jake leaves the Bushwhackers, it's not because he saw error in his way, he certainly doesn't give himself over to the virtue of the cause of abolition.\n",
      "Sentiment: positive\n",
      "Text: this finally drives home the film's other big flaw: lack of originality. In this review, I realize it's been far too easy to reference many other films. Granted, this film is an improvement on most of them, but still. *The Secret Lives of Dentists* is worth seeing, but don't get too excited about it. (Not that you were all that excited, anyway. I guess.)\n",
      "Sentiment: negative\n",
      "Text: There is no relation at all between Fortier and Profiler but the fact that both are police series about violent crimes. Profiler looks crispy, Fortier looks classic. Profiler plots are quite simple. Fortier's plot are far more complicated... Fortier looks more like Prime Suspect, if we have to spot similarities... The main character is weak and weirdo, but have \"clairvoyance\". People like to compare, to judge, to evaluate. How about just enjoying? Funny thing too, people writing Fortier looks American but, on the other hand, arguing they prefer American series (!!!). Maybe it's the language, or the spirit, but I think this series is more English than American. By the way, the actors are really good and funny. The acting is not superficial at all... The sentiment is\n"
     ]
    }
   ],
   "source": [
    "demonstration = create_demonstrations(\"Classify the sentiment of the text.\", small_demonstration_set)\n",
    "print(f\"Demonstration:\\n{demonstration}\")\n",
    "\n",
    "train_prompts = create_prompts(small_train_dataset[\"text\"], demonstration)\n",
    "test_prompts = create_prompts(small_test_dataset[\"text\"], demonstration)\n",
    "print(f\"Prompt Example:\\n{train_prompts[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation Activations for Module Name layers.10\n",
      "Generating Activations with Prompts: train\n",
      "Batch 50 Completed\n",
      "Batch 100 Completed\n",
      "Generating Activations with Prompts: test\n",
      "Batch 50 Completed\n",
      "Batch 100 Completed\n",
      "Batch 150 Completed\n",
      "Batch 200 Completed\n",
      "Batch 250 Completed\n",
      "Batch 300 Completed\n",
      "Generation Activations for Module Name layers.20\n",
      "Generating Activations with Prompts: train\n",
      "Batch 50 Completed\n",
      "Batch 100 Completed\n",
      "Generating Activations with Prompts: test\n",
      "Batch 50 Completed\n",
      "Batch 100 Completed\n",
      "Batch 150 Completed\n",
      "Batch 200 Completed\n",
      "Batch 250 Completed\n",
      "Batch 300 Completed\n",
      "Generation Activations for Module Name layers.30\n",
      "Generating Activations with Prompts: train\n",
      "Batch 50 Completed\n",
      "Batch 100 Completed\n",
      "Generating Activations with Prompts: test\n",
      "Batch 50 Completed\n",
      "Batch 100 Completed\n",
      "Batch 150 Completed\n",
      "Batch 200 Completed\n",
      "Batch 250 Completed\n",
      "Batch 300 Completed\n",
      "Generation Activations for Module Name layers.39\n",
      "Generating Activations with Prompts: train\n",
      "Batch 50 Completed\n",
      "Batch 100 Completed\n",
      "Generating Activations with Prompts: test\n",
      "Batch 50 Completed\n",
      "Batch 100 Completed\n",
      "Batch 150 Completed\n",
      "Batch 200 Completed\n",
      "Batch 250 Completed\n",
      "Batch 300 Completed\n"
     ]
    }
   ],
   "source": [
    "layer_numbers = [\"10\", \"20\", \"30\", \"39\"]\n",
    "module_names = [f\"layers.{layer_number}\" for layer_number in layer_numbers]\n",
    "\n",
    "train_labels = small_train_dataset[\"label\"]\n",
    "test_labels = small_test_dataset[\"label\"]\n",
    "\n",
    "assert len(module_names) == len(layer_numbers)\n",
    "\n",
    "for module_name, layer_number in zip(module_names, layer_numbers):\n",
    "    print(f\"Generation Activations for Module Name {module_name}\")\n",
    "    generate_dataset_activations(\n",
    "        \"train\", train_prompts, train_labels, module_name, f\"_activations_with_prompts_demo_{layer_number}\"\n",
    "    )\n",
    "    generate_dataset_activations(\n",
    "        \"test\",\n",
    "        test_prompts,\n",
    "        test_labels,\n",
    "        module_name,\n",
    "        f\"_activations_with_prompts_demo_{layer_number}\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these activations saved, the next step is to train a simple classifier on top of them in order to perform the sentiment classification. This is done in the `train_on_activations.ipynb` notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
